{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import functools\n",
    "import hashlib\n",
    "import inspect\n",
    "import re\n",
    "import toml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def printrw(*args, **kwargs):\n",
    "    '''fancy reproduce.work print function'''\n",
    "    # if the first arg is a string, prepend it with ╔ω\n",
    "    if len(args) > 0 and isinstance(args[0], str):\n",
    "        args = (\"╔ω: \"+args[0], *args[1:])\n",
    "\n",
    "    # for each arg, replace newlines with ╚\n",
    "    if len(args) > 0:\n",
    "        new_args = []\n",
    "        for a in args[:-1]:\n",
    "            if isinstance(a, str):  \n",
    "                a = a.replace(\"\\n\", \"\\n║ \")\n",
    "\n",
    "            new_args.append(a)\n",
    "        if isinstance(args[:-1], str): \n",
    "            new_args.append(args[:-1].replace(\"\\n\", \"\\n╚ \"))\n",
    "        else:\n",
    "            new_args.append(args[-1])\n",
    "        args = tuple(new_args)\n",
    "\n",
    "    print(*args, **kwargs, flush=True)\n",
    "\n",
    "\n",
    "\n",
    "def set_default_dir():\n",
    "    if not os.getenv(\"REPROWORKDIR\", False):\n",
    "        dir_ = Path(\"./reproduce\")\n",
    "    else:\n",
    "        dir_ = os.getenv(\"REPROWORKDIR\")\n",
    "    return dir_\n",
    "                \n",
    "reproduce_dir = set_default_dir()\n",
    "printrw(f'Setting reproduce.work config dir to {reproduce_dir}')\n",
    "\n",
    "def find_project_path():\n",
    "    current_dir = Path().resolve()\n",
    "\n",
    "    project_path = None\n",
    "    for directory in [current_dir] + list(current_dir.parents):\n",
    "        config_file = directory / reproduce_dir / 'config.toml'\n",
    "        if config_file.is_file():\n",
    "            project_path = directory\n",
    "            break\n",
    "\n",
    "    if not project_path:\n",
    "        raise Exception(f\"Could not find config.toml in any parent directory of {current_dir}; ensure you have run `rw init` in the root of your project and that you are running this command from within your project directory.\")\n",
    "    \n",
    "    return project_path\n",
    "\n",
    "\n",
    "project_path = find_project_path()\n",
    "\n",
    "\n",
    "\n",
    "def read_base_config(return_project_path=False):\n",
    "\n",
    "    project_path = find_project_path()\n",
    "    config_loc = project_path / reproduce_dir / 'config.toml'\n",
    "\n",
    "    with open(config_loc, 'r') as f:\n",
    "        base_config = toml.load(f)\n",
    "    return base_config\n",
    "\n",
    "    \n",
    "def toml_dump(val):\n",
    "    # Convert special types to serializable formats\n",
    "    def serialize_special_types(obj):\n",
    "        if isinstance(obj, pd.Timestamp):\n",
    "            return obj.isoformat()\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, pd.DataFrame):\n",
    "            return obj.to_dict(orient='records')\n",
    "        elif obj is None:\n",
    "            return 'None'\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: serialize_special_types(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [serialize_special_types(i) for i in obj]\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    serialized_val = serialize_special_types(val)\n",
    "    \n",
    "    return toml.loads(toml.dumps({'val': serialized_val}))['val']\n",
    "\n",
    "class ReproduceWorkEncoder(toml.TomlEncoder):\n",
    "    def dump_str(self, v):\n",
    "        \"\"\"Encode a string.\"\"\"\n",
    "        if \"\\n\" in v:\n",
    "            return '\"\"\"\\n' + v.strip() + '\\n' + '\"\"\"'\n",
    "        return super().dump_str(v)\n",
    "    \n",
    "    def dump_value(self, v):\n",
    "        \"\"\"Determine the type of a Python object and serialize it accordingly.\"\"\"\n",
    "        if isinstance(v, str) and \"\\n\" in v:\n",
    "            return '\"\"\"\\n' + v.strip() + '\\n' + '\"\"\"'\n",
    "        return super().dump_value(v)\n",
    "\n",
    "\n",
    "def validate_base_config(base_config, quiet=False):\n",
    "    required_keys = ['authors', 'repro']\n",
    "    for key in required_keys:\n",
    "        if key not in base_config:\n",
    "            #printrw(toml.dumps(base_config))\n",
    "            if not quiet:\n",
    "                raise Exception(f\"Error with ╔ω config: Missing required field '{key}' in config.toml\")\n",
    "            return False\n",
    "        if key=='repro':\n",
    "            stages = ['build', 'develop', 'assemble'] #base_config['repro']['stages']\n",
    "            for stage in stages:\n",
    "                if (f'repro.stage.{stage}' not in base_config) and (stage not in base_config['repro']['stage']):\n",
    "                    if not quiet:\n",
    "                        (toml.dumps(base_config, encoder=ReproduceWorkEncoder()))\n",
    "                    raise Exception(f\"Error with ╔ω config:: Missing required field repro.stage.{stage} in reproduce.work configuration at {reproduce_dir}/config.toml\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def requires_config(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            config = read_base_config()\n",
    "        except:\n",
    "            print(os.getcwd())\n",
    "            print(os.listdir())\n",
    "            raise Exception(\"Your reproduce.work config is either missing or invalid. Run generate_config() to generate a config file.\")\n",
    "        if not validate_base_config(config):\n",
    "            raise Exception(\"Your reproduce.work configuration is not valid.\")\n",
    "        if func.__name__ in [\"publish_data\",\"publish_file\"] and VAR_REGISTRY['REPROWORK_REMOTE_URL'] is None:\n",
    "            msg = (\n",
    "                \"When using publish_data or publish_file, you must first run register_notebook('code/<path to this notebook>.ipynb')\" +\n",
    "                \"to register this session with reproduce.work. If you have multiple notebooks open simultaneously, keep in mind that\" +\n",
    "                \"only the most recently registered notebook will be used as the generating script for any data published with publish_data or publish_file.\"\n",
    "            )\n",
    "            raise Exception(msg)\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "VAR_REGISTRY = {\n",
    "    'REPROWORK_REMOTE_URL': None,\n",
    "    'REPROWORK_ACTIVE_NOTEBOOK': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "def update_watched_files(add=[], remove=[], quiet=False):\n",
    "    base_config = read_base_config()\n",
    "    existing_files = base_config['repro']['files']['watch']\n",
    "    new_files = existing_files + [a for a in add if a not in existing_files]\n",
    "    new_files = [f for f in new_files if f not in remove]\n",
    "    base_config['repro']['files']['watch'] = new_files\n",
    "\n",
    "    current_develop_script = base_config['repro']['stage']['develop']['script']\n",
    "    \n",
    "    # regex to replace content in string matching 'watcher \\\"{to_replace}\\\"'\n",
    "    # with 'watcher \\\"{new_files}\\\"'\n",
    "    # and replace 'build_cmd' with 'python reproduce_work.build()'\n",
    "    import re\n",
    "    new_develop_script = re.sub(\n",
    "        r'watcher \\\"(.*?)\\\"', \n",
    "        f'watcher \\\"{\",\".join([f.strip() for f in new_files])}\\\"'.strip().rstrip(\",\"), \n",
    "        current_develop_script\n",
    "    )\n",
    "    base_config['repro']['stage']['develop']['script'] = new_develop_script\n",
    "\n",
    "    if current_develop_script!=new_develop_script:\n",
    "        with open(Path(reproduce_dir, 'config.toml'), 'w') as f:\n",
    "            toml.dump(base_config, f, encoder=ReproduceWorkEncoder())\n",
    "            \n",
    "        if base_config['repro'].get('verbose', False) and not quiet:\n",
    "            printrw(f\"Updated watched files to {new_files}\")\n",
    "\n",
    "    return new_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "class PublishedObj:\n",
    "    def __init__(self, key, metadata=None, filepath=None, value=None):\n",
    "        self.key = key\n",
    "        self._metadata = metadata or {}\n",
    "        self._type = self._metadata.get('type', None)\n",
    "        self.value = value or self._metadata.get('value', None)\n",
    "        self.embedded_link = self._metadata.get('published_url', None)\n",
    "        \n",
    "        self._content = None\n",
    "        self._str_content = None\n",
    "\n",
    "        # Check if this is a file type and then load content if possible\n",
    "        if self._type == 'file':\n",
    "            self.filepath = filepath\n",
    "            self.load_file_content()\n",
    "            if self._str_content:\n",
    "                self.value = self._str_content\n",
    "        \n",
    "        elif self._type == 'data':\n",
    "            self.value = self._metadata.get('value', None)\n",
    "        \n",
    "        else:\n",
    "            self.value = self._metadata.get('value', None)\n",
    "            \n",
    "    def __call__(self):\n",
    "        return self.value\n",
    "\n",
    "    def load_file_content(self):\n",
    "        project_path = find_project_path()\n",
    "        # Check if file exists\n",
    "        if (project_path / self.filepath).exists():\n",
    "            try:\n",
    "                with open(project_path / self.filepath, 'rb') as f:\n",
    "                    self._content = f.read()\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Error reading file: {e}\")\n",
    "\n",
    "            # If the file extension indicates it's a text type, load as string\n",
    "            if self.filepath.endswith(('.txt', '.csv', '.json')):  # add more text file extensions as needed\n",
    "                self._str_content = self._content.decode('utf-8', errors='replace')\n",
    "\n",
    "    def get_embedded_link(self):\n",
    "        #printrw('loading embedded link: ' + self._metadata['published_url'] + '#' + self.key)\n",
    "        self.embedded_link = self._metadata['published_url'] + '#' + self.key\n",
    "        return self.embedded_link\n",
    "\n",
    "    @property\n",
    "    def content(self):\n",
    "        return self._content\n",
    "\n",
    "    @property\n",
    "    def str_content(self):\n",
    "        return self._str_content\n",
    "\n",
    "    @property\n",
    "    def metadata(self):\n",
    "        return self._metadata\n",
    "\n",
    "\n",
    "\n",
    "def check_for_embedded_objects(metadata, current_path=None, existing_results=None):\n",
    "    if existing_results is None:\n",
    "        result = {}\n",
    "    else:\n",
    "        result = existing_results.copy()\n",
    "    \n",
    "    # Initialize current_path as an empty list if it's None\n",
    "    if current_path is None:\n",
    "        current_path = []\n",
    "\n",
    "    # Check if metadata is a list\n",
    "    if isinstance(metadata, list):\n",
    "        for idx, item in enumerate(metadata):\n",
    "            new_path = current_path + [str(idx)]\n",
    "            # If item is a dictionary or another list, check recursively\n",
    "            if isinstance(item, (dict, list)):\n",
    "                new_result = check_for_embedded_objects(item, current_path=new_path, existing_results=result)\n",
    "                result.update(new_result)\n",
    "            # If item is an instance of PublishedObj, process it\n",
    "            elif isinstance(item, PublishedObj):\n",
    "                key = \".\".join(new_path)\n",
    "                # Process as per original logic\n",
    "                keys = key.split('.')\n",
    "                current_result = result\n",
    "                for k in keys[:-1]:\n",
    "                    if k not in current_result:\n",
    "                        current_result[k] = {}\n",
    "                    current_result = current_result[k]\n",
    "                current_result[keys[-1]] = item#.metadata['published_url']\n",
    "    elif isinstance(metadata, dict):\n",
    "        for k, v in metadata.items():\n",
    "            new_path = current_path + [k]\n",
    "            \n",
    "            if isinstance(v, PublishedObj):\n",
    "                # Generate a key based on the current path\n",
    "                key = \".\".join(new_path)\n",
    "                # Store the metadata\n",
    "                keys = key.split('.')\n",
    "                current_result = result\n",
    "                for k in keys[:-1]:\n",
    "                    if k not in current_result:\n",
    "                        current_result[k] = {}\n",
    "                    current_result = current_result[k]\n",
    "                current_result[keys[-1]] = v#.metadata['published_url']\n",
    "\n",
    "            elif isinstance(v, (dict, list)):\n",
    "                # check recursively\n",
    "                new_result = check_for_embedded_objects(v, current_path=new_path, existing_results=result)\n",
    "                # Merge new results into the existing result dictionary\n",
    "                result.update(new_result)\n",
    "\n",
    "    else:\n",
    "        raise Exception(f'Unknown metadata type: {type(metadata)}')\n",
    "\n",
    "    return result\n",
    "\n",
    "def replace_with_embedded_links(obj):\n",
    "    \"\"\"\n",
    "    Recursively replace PublishedObj instances with the result of their get_embedded_link method.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        for key, value in obj.items():\n",
    "            if isinstance(value, PublishedObj):\n",
    "                obj[key] = value.get_embedded_link()\n",
    "            else:\n",
    "                replace_with_embedded_links(value)\n",
    "    elif isinstance(obj, list):\n",
    "        for idx, item in enumerate(obj):\n",
    "            if isinstance(item, PublishedObj):\n",
    "                obj[idx] = item.get_embedded_link()\n",
    "            else:\n",
    "                replace_with_embedded_links(item)\n",
    "\n",
    "\n",
    "@requires_config\n",
    "def publish_data(content, name, metadata={}, watch=True, force=False):\n",
    "    \"\"\"\n",
    "    Save data to default pubdata.toml file and register metadata.\n",
    "    # publishing the exact same data twice will NOT update the pubdata.toml file\n",
    "    \"\"\"\n",
    "    base_config = read_base_config()\n",
    "    pubdata_relpath = Path(base_config['repro']['files']['dynamic'])\n",
    "    pubdata_fullpath = find_project_path() / pubdata_relpath\n",
    "\n",
    "    # handle any embedded objects:\n",
    "    # recurse through metadata and find any objects that are of type PublishedObj\n",
    "    # and print out their names\n",
    "    embedded_objects = check_for_embedded_objects(metadata)\n",
    "    if embedded_objects:\n",
    "        metadata_copy = metadata.copy()\n",
    "        replace_with_embedded_links(metadata_copy)\n",
    "        metadata = metadata_copy.copy()\n",
    "    \n",
    "    # Capture dynamic metadata\n",
    "    timestamp = datetime.datetime.now().isoformat()\n",
    "    inspect_filename = inspect.currentframe().f_back.f_code.co_filename\n",
    "    #python_version = sys.version.strip().replace('\\n', ' ')\n",
    "    #platform_info = platform.platform()\n",
    "\n",
    "    # generate cryptographic hash of file contents\n",
    "    content_hash = hashlib.md5(str(content).encode('utf-8')).hexdigest()\n",
    "    timed_hash = hashlib.md5((str(content) + timestamp).encode('utf-8')).hexdigest()\n",
    "         \n",
    "    # Store metadata\n",
    "    new_metadata = {\n",
    "        \"type\": \"data\",\n",
    "        \"timestamp\": timestamp,\n",
    "        \"content_hash\": content_hash,\n",
    "        \"timed_hash\": timed_hash,\n",
    "        #\"python_version\": python_version,\n",
    "        #\"platform_info\": platform_info,\n",
    "    }\n",
    "    if VAR_REGISTRY['REPROWORK_REMOTE_URL']:\n",
    "        metadata['published_url'] = f\"{VAR_REGISTRY['REPROWORK_REMOTE_URL']}/{reproduce_dir}/pubdata.toml\"\n",
    "    else:\n",
    "        metadata['published_url'] = f\"{Path(reproduce_dir, 'pubdata.toml').resolve().as_posix()}\".replace('/home/jovyan/', '')\n",
    "\n",
    "    if VAR_REGISTRY['REPROWORK_ACTIVE_NOTEBOOK']:\n",
    "        metadata['generating_script'] = VAR_REGISTRY['REPROWORK_ACTIVE_NOTEBOOK']\n",
    "    else:\n",
    "        metadata['generating_script'] = inspect_filename.replace('/home/jovyan/', '')\n",
    "\n",
    "    metadata.update(new_metadata)\n",
    "\n",
    "    for k,v in metadata.items():\n",
    "        if isinstance(v, dict):\n",
    "            #printrw('Dumping dict w/ toml')\n",
    "            metadata[k] = rf'{toml_dump(v)}'\n",
    "    \n",
    "    metadata['value'] = f'{toml_dump(content)}'\n",
    "\n",
    "    \"\"\"\n",
    "    if metadata.get('type', '') == 'text/latex':\n",
    "        # escape special characters\n",
    "        #metadata['value'] = content.replace('\\\\', '\\\\\\\\').replace('&', '\\\\&').replace('$', '\\$')\n",
    "        pass\n",
    "    elif isinstance(content, dict):\n",
    "        metadata['value'] = f'''{toml_dump(content)}'''\n",
    "    else:\n",
    "        metadata['value'] = content\n",
    "    \"\"\"\n",
    "\n",
    "    if watch:\n",
    "        update_watched_files(add=[Path(reproduce_dir, 'pubdata.toml').resolve().as_posix().replace('/home/jovyan/', '')])\n",
    "\n",
    "    # check if dynamic file exists\n",
    "    if not os.path.exists(pubdata_fullpath):\n",
    "        with open(pubdata_fullpath, 'w') as file:\n",
    "            file.write(toml.dumps({}))\n",
    "        existing_dynamic_data = {}\n",
    "    else:\n",
    "        with open(pubdata_fullpath, 'r') as file:\n",
    "            existing_dynamic_data = toml.load(file)\n",
    "        \n",
    "    dynamic_data = existing_dynamic_data.copy()\n",
    "    dynamic_data[name] = metadata\n",
    "\n",
    "    existing_vals = []\n",
    "    for k,v in existing_dynamic_data.items():\n",
    "        if isinstance(v, dict):\n",
    "            if 'value' in v:\n",
    "                existing_vals.append({k:v['value']})\n",
    "    \n",
    "    new_vals = []\n",
    "    for k,v in dynamic_data.items():\n",
    "        if isinstance(v, dict):\n",
    "            if 'value' in v:\n",
    "                new_vals.append({k:v['value']})\n",
    "\n",
    "\n",
    "    def is_equal(a, b):\n",
    "        try:\n",
    "            a = toml_dump(a)\n",
    "            b = toml_dump(b)\n",
    "            # Handle NumPy arrays\n",
    "            if isinstance(a, np.ndarray) or isinstance(b, np.ndarray):\n",
    "                return np.array_equal(a, b)\n",
    "            \n",
    "            # Handle lists and tuples\n",
    "            elif isinstance(a, (list, tuple)) and isinstance(b, (list, tuple)):\n",
    "                return len(a) == len(b) and all([is_equal(x, y) for x, y in zip(a, b)])\n",
    "            \n",
    "            # For other types, attempt a direct comparison\n",
    "            else:\n",
    "                return a == b\n",
    "            \n",
    "        except ValueError:\n",
    "            # If direct comparison raises a ValueError, assume they're not equal\n",
    "            printrw('ValueError: could not compare values: {} and {}'.format(a, b))\n",
    "            return False\n",
    "\n",
    "    #value_changed = any([not is_equal(v, new_vals[i]) for i, v in enumerate(existing_vals)])\n",
    "    #iterate through existing_vals and new_vals and check if any have changed\n",
    "    changed_vals = []\n",
    "    for i, v in enumerate(existing_vals):\n",
    "        if i<len(new_vals):\n",
    "            #if objects are identical, return False; otherwise return name(s) of variable(s) that changed\n",
    "            if not is_equal(v, new_vals[i]):\n",
    "                changed_vals.append(name)\n",
    "\n",
    "    # do same for reverse\n",
    "    for i, v in enumerate(new_vals):\n",
    "        if i<len(existing_vals):\n",
    "            if not is_equal(v, existing_vals[i]):\n",
    "                changed_vals.append(name)\n",
    "    \n",
    "    changed_vals = list(set(changed_vals))\n",
    "\n",
    "    if changed_vals:\n",
    "        value_changed = changed_vals\n",
    "    else:\n",
    "        value_changed = False\n",
    "    \n",
    "\n",
    "    existing_nontimefields = {name: {k:v for k,v in var.items() if (isinstance(k, str) and k[:4] not in ['time','valu'])} if isinstance(var, dict) else var for name,var in existing_dynamic_data.items()}\n",
    "    new_nontimefields = {name: {k:v for k,v in var.items() if (isinstance(k, str) and k[:4] not in ['time','valu'])} if isinstance(var, dict) else var for name,var in dynamic_data.items()}\n",
    "    which_changed = []\n",
    "    \n",
    "    if existing_nontimefields == new_nontimefields:\n",
    "        non_timefield_changed = False\n",
    "    else:\n",
    "        for name in existing_nontimefields:\n",
    "            if name in new_nontimefields:\n",
    "                if existing_nontimefields[name] != new_nontimefields[name]:\n",
    "                    which_changed.append(name)\n",
    "            else:\n",
    "                which_changed.append(name)\n",
    "        \n",
    "        for name in new_nontimefields:\n",
    "            if name not in existing_nontimefields:\n",
    "                which_changed.append(name)\n",
    "        \n",
    "        non_timefield_changed = which_changed\n",
    "\n",
    "    if value_changed:\n",
    "        which_changed = value_changed + which_changed\n",
    "\n",
    "\n",
    "    data_obj = PublishedObj(name, metadata)\n",
    "    data_obj.get_embedded_link()\n",
    "        \n",
    "    if value_changed or non_timefield_changed or force:\n",
    "        with open(pubdata_fullpath, 'w') as file:\n",
    "            toml.dump(dynamic_data, file, encoder=ReproduceWorkEncoder())\n",
    "\n",
    "        print(f'Published data in {pubdata_relpath}')\n",
    "        print('    generating_script: ' + data_obj.metadata['generating_script'])\n",
    "        print('    timed_hash: ' + data_obj.metadata['timed_hash'])\n",
    "\n",
    "        #if base_config['repro'].get('verbose', False):\n",
    "        #    if len(which_changed)>1:\n",
    "        #        printrw(f\"Updated {which_changed} in {pubdata_relpath}\")\n",
    "        #    else:\n",
    "        #        printrw(f\"Updated {which_changed[0]} in {pubdata_relpath}\")\n",
    "\n",
    "    else:\n",
    "        print(f'Data already published in {pubdata_relpath} and value unchanged; use force=True to overwrite.')\n",
    "        print('    generating_script: ' + data_obj.metadata['generating_script'])\n",
    "        print('    timed_hash: ' + data_obj.metadata['timed_hash'])\n",
    "\n",
    "    process_pubdata_links(verbose=False)\n",
    "    process_pubdata_links(verbose=False)\n",
    "\n",
    "    return data_obj\n",
    "\n",
    "def generate_filepath_key(path):\n",
    "    # Extract the filename from the path\n",
    "    filename = path.split(\"/\")[-1]\n",
    "    \n",
    "    # Replace slashes, dots, and underscores with TOML-compatible delimiters\n",
    "    transformed_filename = filename.replace(\"_\", \"_us_\").replace(\"/\", \"__\").replace(\".\", \"_dot_\")\n",
    "    \n",
    "    # Compute the hash of the entire path\n",
    "    hash_value = hashlib.sha256(path.encode()).hexdigest()\n",
    "\n",
    "    # Chop off everything after and including \"_dot_\"\n",
    "    keyname = transformed_filename.split(\"_dot_\")[0]\n",
    "    \n",
    "    # Replace \"_us_\" with \"_\"\n",
    "    keyname = keyname.replace(\"_us_\", \"_\")\n",
    "    \n",
    "    # Combine the transformed filename with the first 8 characters of the hash\n",
    "    key = f\"{keyname}_{hash_value[:8]}\"\n",
    "    \n",
    "    return key\n",
    "\n",
    "\n",
    "def get_cell_index():\n",
    "    \"\"\"\n",
    "    Get the current cell index in a Jupyter notebook environment.\n",
    "    If not in Jupyter, return None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Execute JavaScript to get the current cell index\n",
    "        from IPython import get_ipython\n",
    "        get_ipython().run_cell_magic('javascript', '', 'IPython.notebook.kernel.execute(\\'current_cell_index = \\' + IPython.notebook.get_selected_index())')\n",
    "        return current_cell_index\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "\n",
    "@requires_config\n",
    "def publish_file(filepath, key=None, metadata={}, watch=True):\n",
    "    \"\"\"\n",
    "    Save content to a file and register metadata.\n",
    "    \"\"\"\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "    \n",
    "    project_path = find_project_path()\n",
    "    base_config = read_base_config()\n",
    "    pubdata_relpath = Path(base_config['repro']['files']['dynamic'])\n",
    "    pubdata_fullpath = project_path / pubdata_relpath\n",
    "\n",
    "    if key is None:\n",
    "        key = generate_filepath_key(filepath)\n",
    "\n",
    "\n",
    "    embedded_objects = check_for_embedded_objects(metadata)\n",
    "    if embedded_objects:\n",
    "        #print(metadata)\n",
    "        metadata_copy = metadata.copy()\n",
    "        replace_with_embedded_links(metadata_copy)\n",
    "        metadata = metadata_copy.copy()\n",
    "        #print(metadata)\n",
    "\n",
    "    for k,v in metadata.items():\n",
    "        if isinstance(v, dict):\n",
    "            #printrw('Dumping dict w/ toml')\n",
    "            metadata[k] = rf'{toml_dump(v)}'\n",
    "\n",
    "    # Capture metadata\n",
    "    timestamp = datetime.datetime.now().isoformat()\n",
    "    \n",
    "    if 'co_filepath' in inspect.currentframe().f_back.f_code.__dir__():\n",
    "        # check if in Jupyter environment has co_filepath\n",
    "        inspect_filepath = inspect.currentframe().f_back.f_code.co_filepath\n",
    "    else:\n",
    "        inspect_filepath = ''\n",
    "\n",
    "    #python_version = sys.version.strip().replace('\\n', ' ')\n",
    "    #platform_info = platform.platform()\n",
    "\n",
    "    # generate cryptographic hash of file contents\n",
    "    file_fullpath = project_path / filepath\n",
    "    if not file_fullpath.exists():\n",
    "        raise Exception(f\"Could not find file {filepath}; ensure the file is in your project's directory and you are using the exact path from the project root to the saved file.\")\n",
    "    \n",
    "    with open(file_fullpath, 'rb') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    content_hash = hashlib.md5(content).hexdigest()\n",
    "    timed_hash = hashlib.md5((content_hash + timestamp).encode('utf-8')).hexdigest()\n",
    "         \n",
    "    #save_context, definition_context = check_for_defintion_in_context(function_name='save')\n",
    "\n",
    "    # Store metadata\n",
    "    new_metadata = {\n",
    "        \"type\": \"file\",\n",
    "        \"filepath\": filepath,\n",
    "        \"timestamp\": timestamp,\n",
    "        #\"python_version\": python_version,\n",
    "        #\"platform_info\": platform_info,\n",
    "        \"content_hash\": content_hash,\n",
    "        \"timed_hash\": timed_hash\n",
    "        #\"save_context\": save_context,\n",
    "        #\"definition_context\": definition_context\n",
    "    }\n",
    "    #cell_index = get_cell_index()\n",
    "    #if cell_index:\n",
    "    #    new_metadata[\"cell_index\"] = cell_index\n",
    "    \n",
    "    metadata.update(new_metadata)\n",
    "\n",
    "    if watch:\n",
    "        update_watched_files(add=[filepath])\n",
    "\n",
    "    # check if dynamic file exists\n",
    "    if not os.path.exists(pubdata_fullpath):\n",
    "        with open(Path(pubdata_fullpath), 'w') as file:\n",
    "            dynamic_data = {}\n",
    "            file.write(toml.dumps(dynamic_data))\n",
    "    else:\n",
    "        with open(pubdata_fullpath, 'r') as file:\n",
    "            dynamic_data = toml.load(file)\n",
    "\n",
    "    if VAR_REGISTRY['REPROWORK_REMOTE_URL']:\n",
    "        metadata['published_url'] = f\"{VAR_REGISTRY['REPROWORK_REMOTE_URL']}/{pubdata_relpath}\"\n",
    "        metadata['content_url'] = f\"{VAR_REGISTRY['REPROWORK_REMOTE_URL']}/{filepath}\"\n",
    "    else:\n",
    "        metadata['published_url'] = base_config['repro']['files']['dynamic']\n",
    "        metadata['content_url'] = filepath\n",
    "\n",
    "    if VAR_REGISTRY['REPROWORK_ACTIVE_NOTEBOOK']:\n",
    "        metadata['generating_script'] = VAR_REGISTRY['REPROWORK_ACTIVE_NOTEBOOK']\n",
    "    else:\n",
    "        metadata['generating_script'] = inspect_filename.replace('/home/jovyan/', '')\n",
    "\n",
    "    dynamic_data[key] = metadata\n",
    "\n",
    "    #printrw(dynamic_data)\n",
    "\n",
    "    with open(pubdata_fullpath, 'w') as file:\n",
    "        toml.dump(dynamic_data, file, encoder=ReproduceWorkEncoder())\n",
    "\n",
    "    #if 'verbosity' in base_config['repro'] and base_config['repro']['verbose']:\n",
    "    #    printrw(f\"Added metadata for file {filepath} to dynamic file {pubdata_relpath}\")\n",
    "\n",
    "    data_obj = PublishedObj(key, metadata=metadata, filepath=filepath)\n",
    "    data_obj.get_embedded_link()\n",
    "\n",
    "    print(f'Published metadata for file in {pubdata_relpath}')\n",
    "    print('    generating_script: ' + data_obj.metadata['generating_script'])\n",
    "    print('    timed_hash: ' + data_obj.metadata['timed_hash'])\n",
    "\n",
    "    process_pubdata_links(verbose=False)\n",
    "    process_pubdata_links(verbose=False)\n",
    "    \n",
    "    return data_obj\n",
    "\n",
    "\n",
    "\n",
    "@requires_config\n",
    "def register_notebook(notebook_path, notebook_dir=None, quiet=False):\n",
    "    \"\"\"\n",
    "    Register a notebook to the config.toml file.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_config = read_base_config()\n",
    "    project_path = find_project_path()\n",
    "\n",
    "    if not (project_path / notebook_path).exists():\n",
    "        raise Exception(f\"Error: Notebook '{notebook_path}' does not exist. Ensure the notebook you are trying to register is in your project's directory and you are using the exact path to the current notebook (relative to your project's root fodler).\")\n",
    "    \n",
    "    if 'repository' in base_config['project']:\n",
    "        remote_url_val = f\"{base_config['project']['repository']}/blob/main\"\n",
    "        notebook_new_val = f\"{remote_url_val}/{notebook_path}\"\n",
    "    else:\n",
    "        remote_url_val = ''\n",
    "        notebook_new_val = Path(notebook_path).resolve().as_posix().replace('/home/jovyan/', '')\n",
    "    \n",
    "    if VAR_REGISTRY['REPROWORK_REMOTE_URL'] is None:\n",
    "        printrw(f\"Registered notebook {notebook_new_val} in {reproduce_dir}/config.toml\")\n",
    "\n",
    "    elif remote_url_val!=VAR_REGISTRY['REPROWORK_REMOTE_URL']:\n",
    "        printrw(f\"Warning: {VAR_REGISTRY['REPROWORK_REMOTE_URL']} is already registered. Overwriting with {remote_url_val}\")\n",
    "    VAR_REGISTRY['REPROWORK_REMOTE_URL'] = remote_url_val\n",
    "\n",
    "    if VAR_REGISTRY['REPROWORK_ACTIVE_NOTEBOOK']:\n",
    "        printrw(f\"Warning: Notebook {VAR_REGISTRY['REPROWORK_ACTIVE_NOTEBOOK']} is already registered. Overwriting with {notebook_new_val}\")\n",
    "    VAR_REGISTRY['REPROWORK_ACTIVE_NOTEBOOK'] = notebook_new_val\n",
    "    \n",
    "    #update_watched_files(add=[notebook_path], quiet=True)\n",
    "    if 'registered' not in base_config['repro']['files']:\n",
    "        base_config['repro']['files']['registered'] = [notebook_path]\n",
    "    elif notebook_path not in base_config['repro']['files']['registered']:\n",
    "        base_config['repro']['files']['registered'].append(notebook_path)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "def find_pubdata_links():\n",
    "    project_path = find_project_path()\n",
    "    base_config = read_base_config()\n",
    "    dynamic_file = base_config['repro']['files']['dynamic']\n",
    "\n",
    "    with open(project_path / dynamic_file, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Adjusted pattern to capture optional #hash\n",
    "    pattern = r\"['\\\"]?(\\w+)['\\\"]?\\s*[:=]\\s*(?:\\[\\s*)?['\\\"]?([^'\\\"]*pubdata\\.toml(?:#([\\w\\d_\\-]+))?)[\\\"']?\"\n",
    "\n",
    "    results = []\n",
    "    lines = content.splitlines()\n",
    "    for match in re.finditer(pattern, content):\n",
    "        # Extract matched data\n",
    "        var_name = match.group(1)\n",
    "        path = match.group(2)\n",
    "        hash_name = match.group(3)\n",
    "        \n",
    "        # Extract matched path details\n",
    "        start_pos = match.start(2)\n",
    "        end_pos = match.end(2)\n",
    "        line_number = content.count('\\n', 0, start_pos) + 1  # +1 because line numbers are 1-indexed\n",
    "\n",
    "        # Identify contiguous lines before the matched line\n",
    "        start_line = line_number\n",
    "        for i in range(line_number - 1, 0, -1):  # Start from the line before the match\n",
    "            line = lines[i - 1].strip()  # -1 because list indices are 0-indexed\n",
    "            if not line and \"'''\" not in line and '\"\"\"' not in line:\n",
    "                break\n",
    "            start_line = i\n",
    "        \n",
    "        # Identify contiguous lines after the matched line\n",
    "        end_line = line_number\n",
    "        for i in range(line_number, len(lines)):\n",
    "            line = lines[i].strip()\n",
    "            if not line and \"'''\" not in line and '\"\"\"' not in line:\n",
    "                break\n",
    "            end_line = i + 1  # +1 because line numbers are 1-indexed\n",
    "\n",
    "        # Extract the first line of the chunk\n",
    "        toml_header = lines[start_line - 1].strip()  # -1 because list indices are 0-indexed\n",
    "        \n",
    "        result_data = {\n",
    "            \"variable\": var_name,\n",
    "            \"path\": path,\n",
    "            \"start_pos\": start_pos,\n",
    "            \"end_pos\": end_pos,\n",
    "            \"line_range\": (start_line, end_line),\n",
    "            \"toml_header\": toml_header,\n",
    "            \"on_line\": line_number,\n",
    "            'at_char': start_pos - content.rfind('\\n', 0, start_pos),\n",
    "        }\n",
    "        \n",
    "        if hash_name:\n",
    "            result_data[\"hash\"] = hash_name\n",
    "        else:\n",
    "            result_data[\"hash\"] = None\n",
    "        \n",
    "        \n",
    "        results.append(result_data)\n",
    "    \n",
    "    final_results = []\n",
    "    for r in results:\n",
    "        if 'variable' in r.keys() and 'path' in r.keys():\n",
    "            if r['variable'] + r['path'][:2] in ['https//','http//']:\n",
    "                continue\n",
    "        \n",
    "        # if r['hash'] resembles 'L{DIGITS}(\\-L{DIGITS})?', then continue\n",
    "        #if 'hash' in r.keys() and r['hash']:\n",
    "        #    if re.match(r'L\\d+(\\-L\\d+)?', r['hash']):\n",
    "        #        continue\n",
    "            \n",
    "        final_results.append(r)              \n",
    "    \n",
    "    return final_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "\n",
    "def modify_links(linenum, link_str, newlink):\n",
    "\n",
    "    base_config = read_base_config()\n",
    "    pubdata_filename = Path(base_config['repro']['files']['dynamic'])\n",
    "    pubdata_loc = find_project_path() / pubdata_filename\n",
    "\n",
    "    with open(pubdata_loc, 'r') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    lines = content.splitlines()\n",
    "    content_line = lines[linenum-1]\n",
    "\n",
    "    # Define a replacement function that retains the original hash (if present) and appends the linehash\n",
    "    def replacement(match):\n",
    "        # Determine the type of quote used based on the first character of the matched link\n",
    "        quote_type = match.group(1)[0]\n",
    "        \n",
    "        return f'{quote_type}{newlink}{quote_type}'  # Construct the replacement string\n",
    "\n",
    "    # Patterns to match the link (double or single quoted) and capture the hash if present\n",
    "    double_quoted_pattern = rf'(\"{link_str})(#.*?)?\"'\n",
    "    single_quoted_pattern = rf'(\\'{link_str})(#.*?)?\\''\n",
    "\n",
    "    # Apply the replacement for both double and single quoted links\n",
    "    new_content_line = re.sub(double_quoted_pattern, replacement, content_line)\n",
    "    new_content_line = re.sub(single_quoted_pattern, replacement, new_content_line)\n",
    "\n",
    "    lines[linenum-1] = new_content_line\n",
    "    new_content = '\\n'.join(lines)\n",
    "\n",
    "    with open(pubdata_loc, 'w') as f:\n",
    "        f.write(new_content)\n",
    "\n",
    "    return content_line\n",
    "\n",
    "\n",
    "\n",
    "def process_pubdata_links(verbose=False):\n",
    "    #base_config = read_base_config()\n",
    "    #verbose = base_config['repro']['verbose']\n",
    "    publinks = find_pubdata_links()\n",
    "    pldf = pd.DataFrame(publinks)\n",
    "\n",
    "    pldf_og = pldf.copy()\n",
    "\n",
    "    # iterate through find the self-referenential links first,\n",
    "    # i.e., those for which the \"variable\" of the link is \"published_url\"\n",
    "    vars = (pldf.variable=='published_url')\n",
    "    top_level_vars = pldf.loc[vars,'toml_header'].str.slice(1,-1).tolist()\n",
    "    for var in top_level_vars:\n",
    "        # find the linkdata for this variable\n",
    "        linkdata = pldf.query(f\"toml_header=='[{var}]' and variable=='published_url'\")\n",
    "        linkdata = linkdata.iloc[0]\n",
    "\n",
    "        #if end of linkpath already ends with pattern like `#L[numbers]` or `#L[numbers]-L[numbers]`, continue\n",
    "        if re.match(r'.*#L\\d+(\\-L\\d+)?', linkdata.path):\n",
    "            continue\n",
    "\n",
    "        llo, lhi = (linkdata.line_range)\n",
    "        newlink = linkdata.path + f'#L{llo}-L{lhi}'\n",
    "        if verbose:\n",
    "            printrw(f'modify_links({linkdata.on_line}, {linkdata.path}, {newlink})')\n",
    "        modify_links(linkdata.on_line, linkdata.path, newlink)\n",
    "        \n",
    "    publinks = find_pubdata_links()\n",
    "    pldf = pd.DataFrame(publinks)\n",
    "    top_var_links = dict(zip(\n",
    "        pldf.loc[vars,'toml_header'].str.slice(1,-1).tolist(), \n",
    "        pldf.loc[vars,'path'].tolist()\n",
    "    ))\n",
    "\n",
    "    nontop_vars = pldf.loc[~vars,].index.tolist()\n",
    "\n",
    "    for linktext_idx in nontop_vars:\n",
    "        linkdata = pldf.loc[linktext_idx]\n",
    "\n",
    "        #print(linktext_idx, linkdata)\n",
    "\n",
    "        if re.match(r'.*#L\\d+(\\-L\\d+)?', linkdata.path):\n",
    "            continue\n",
    "        \n",
    "        if (\n",
    "            ('hash' in linkdata and linkdata['hash'] in top_var_links.keys()) or \\\n",
    "            ('variable' in linkdata and linkdata['variable'] in top_var_links.keys())\n",
    "        ):\n",
    "            \n",
    "            target_var = linkdata['hash'] if ('hash' in linkdata and linkdata['hash']) else linkdata['variable']\n",
    "            newlink = top_var_links[target_var]\n",
    "\n",
    "            if verbose:\n",
    "                printrw(f'modify_links({linkdata.on_line}, {linkdata.path}, {newlink})')\n",
    "                modify_links(linkdata.on_line, linkdata.path, newlink)\n",
    "\n",
    "        else:\n",
    "            raise Exception(f\"Could not find link target for {linkdata['path']}\")\n",
    "\n",
    "    return pldf_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "## Attempts to get line numbers\n",
    "\n",
    "def reproducible(var_assignment_func):\n",
    "    \"\"\"\n",
    "    A decorator to register the line number and timestamp when a variable is assigned.\n",
    "    \"\"\"\n",
    "    @functools.wraps(var_assignment_func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Extract value and var_name from args\n",
    "        # Assumes the decorated function always takes at least two arguments: value and var_name\n",
    "        value, var_name = args[0], args[1]\n",
    "\n",
    "        # Extract metadata from kwargs or default to an empty dictionary\n",
    "        metadata = kwargs.get('metadata', {})\n",
    "\n",
    "        # Get the current frame and line number\n",
    "        frame = inspect.currentframe()\n",
    "        line_number = frame.f_back.f_lineno\n",
    "\n",
    "        # Get the current timestamp\n",
    "        timestamp = datetime.datetime.now().isoformat()\n",
    "\n",
    "        # Get the filename of the caller\n",
    "        filename = frame.f_back.f_code.co_filename\n",
    "\n",
    "        # Execute the variable assignment function\n",
    "        result = var_assignment_func(*args, **kwargs)\n",
    "\n",
    "        # Register the variable name, line number, timestamp, and filename\n",
    "        VAR_REGISTRY[var_name] = {\n",
    "            \"type\": \"string\",\n",
    "            \"timestamp\": timestamp,\n",
    "        }\n",
    "\n",
    "        if type(value) is not str:\n",
    "            value = str(value)\n",
    "            printrw(f\"WARNING: value of {var_name} was not a string. Converted to string: {value}.\")\n",
    "\n",
    "        VAR_REGISTRY[var_name]['value'] = value\n",
    "\n",
    "        metadata.update(VAR_REGISTRY[var_name])\n",
    "        \n",
    "        if VAR_REGISTRY['REPROWORK_REMOTE_URL']:\n",
    "            metadata['published_url'] = f\"{VAR_REGISTRY['REPROWORK_REMOTE_URL']}/{reproduce_dir}/pubdata.toml\"\n",
    "\n",
    "        if VAR_REGISTRY['REPROWORK_ACTIVE_NOTEBOOK']:\n",
    "            metadata['generating_script'] = VAR_REGISTRY['REPROWORK_ACTIVE_NOTEBOOK']\n",
    "\n",
    "        config = read_base_config()\n",
    "\n",
    "        # check if dynamic file exists\n",
    "        if not os.path.exists(Path(config['repro']['files']['dynamic'])):\n",
    "            with open(Path(config['repro']['files']['dynamic']), 'w') as file:\n",
    "                file.write(toml.dumps({}))\n",
    "        with open(Path(config['repro']['files']['dynamic']), 'r') as file:\n",
    "            dynamic_data = toml.load(file)\n",
    "\n",
    "        dynamic_data[var_name] = metadata\n",
    "\n",
    "        with open(Path(config['repro']['files']['dynamic']), 'w') as file:\n",
    "            toml.dump(dynamic_data, file, encoder=ReproduceWorkEncoder())\n",
    "\n",
    "        return result\n",
    "    return wrapper\n",
    "    \n",
    "def check_for_defintion_in_context(function_name='save'):\n",
    "    assert function_name in ['save', 'assign'], \"function_name must be either 'save' or 'assign'\"\n",
    "    \n",
    "    from IPython import get_ipython\n",
    "    ip = get_ipython()\n",
    "\n",
    "    # Check if in Jupyter environment\n",
    "    if ip is None:\n",
    "        \n",
    "        #fill this in \n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        # Get the input history\n",
    "        #lineno = inspect.stack()[0].lineno\n",
    "        raw_hist = ip.history_manager.input_hist_raw\n",
    "        current_cell = raw_hist[-1]\n",
    "\n",
    "\n",
    "        matches = re.findall(rf\"{function_name}\\((.+?),\", current_cell)\n",
    "                \n",
    "        if matches:\n",
    "            # save call\n",
    "            defined_var = matches[0].strip()\n",
    "            definition_cell_content = ''\n",
    "            \n",
    "            for prior_cell in raw_hist[-2::-1]:\n",
    "                #printrw(prior_cell)\n",
    "                if f'{defined_var} =' in prior_cell or f'{defined_var}=' in prior_cell:\n",
    "                    definition_cell_content = prior_cell\n",
    "                    break\n",
    "            \n",
    "            # find the line number of the where the variable was defined\n",
    "            # Give a window of 5 lines around the definition call\n",
    "            def_cell_lines = definition_cell_content.split('\\n')\n",
    "            if len(def_cell_lines)>0:\n",
    "                lineno = None\n",
    "                for line_num, line in enumerate(def_cell_lines):\n",
    "                    if defined_var in line:\n",
    "                        lineno = line_num\n",
    "                        break\n",
    "                if lineno:\n",
    "                    definition_context = (\n",
    "                        '\\n'.join(def_cell_lines[max(0, lineno-5):lineno]) + \n",
    "                        '\\nFLAG' + def_cell_lines[lineno] + '\\n' +\n",
    "                        '\\n'.join(def_cell_lines[lineno+1:min(len(def_cell_lines), lineno+5)])\n",
    "                    )\n",
    "                else:\n",
    "                    definition_context = None\n",
    "\n",
    "            else:\n",
    "                definition_context = None\n",
    "\n",
    "            \n",
    "            save_cell_lines = current_cell.split('\\n')\n",
    "            if len(save_cell_lines)>0:\n",
    "                save_lineno = None\n",
    "                for line_num, line in enumerate(save_cell_lines):\n",
    "                    if 'save(' in line:\n",
    "                        save_lineno = line_num\n",
    "                        break\n",
    "                \n",
    "                if save_lineno:\n",
    "                    save_context = (\n",
    "                        '\\n'.join(save_cell_lines[max(0, save_lineno-5):save_lineno]) + \n",
    "                        '\\nFLAG' + save_cell_lines[save_lineno] + '\\n' +\n",
    "                        '\\n'.join(save_cell_lines[save_lineno+1:min(len(save_cell_lines), save_lineno+5)])\n",
    "                    )\n",
    "                else:\n",
    "                    save_context = None\n",
    "                \n",
    "            else:\n",
    "                save_context = None\n",
    "            \n",
    "\n",
    "        else:\n",
    "            # not a save call\n",
    "            save_context = None\n",
    "            definition_context = None\n",
    "\n",
    "        return(save_context, definition_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nbdev CI code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import toml\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def construct_config(options):\n",
    "\n",
    "    for key in ['author1_email', 'author1_name', 'author1_affiliation',\n",
    "                'project_full_title', 'project_abstract', 'github_repo_str', \n",
    "                'base_url_str', 'repro_version', 'verbose_str', 'input_file',\n",
    "                'dynamic_file', 'bibfile', 'latex_template', 'watch_files', \n",
    "                'repro_init_script', 'dev_image_tag', 'reproduce_dir',\n",
    "                'output_file', 'output_linefile', 'document_dir', 'nbdev_project_cmd']:\n",
    "        \n",
    "        if key not in options:\n",
    "            printrw(f\"Error: Missing required key '{key}' in options.\")\n",
    "            return False\n",
    "        \n",
    "    newline = \"\\n\"\n",
    "    return f'''\n",
    "[authors]\n",
    "author1.email = \"{options['author1_email']}\"{newline + 'author1.name = \"' + options['author1_name'] + '\"' if options['author1_name'] else \"\"}{newline + 'author1.affiliation = \"' + options['author1_affiliation'] + '\"' if options['author1_affiliation'] else \"\"}\n",
    "\n",
    "[project]\n",
    "full_title = \"{options['project_full_title']}\"\n",
    "abstract = \"\"\"\n",
    "{options['project_abstract']}\n",
    "\"\"\"{options['github_repo_str']}{options['base_url_str']}\n",
    "\n",
    "# reproduce.work configuration\n",
    "[repro]\n",
    "version = \"{options['repro_version']}\"\n",
    "stages = [\"init\", \"develop\", \"build\"]\n",
    "verbose = {options['verbose_str']}\n",
    "document_dir = \"{options['document_dir']}\"\n",
    "\n",
    "[repro.files]\n",
    "input = \"{options['input_file']}\"\n",
    "dynamic = \"{options['dynamic_file']}\"\n",
    "latex_template = \"{options['latex_template']}\"\n",
    "bibfile = \"{options['bibfile']}\"\n",
    "output_linefile = \"{options['output_linefile']}\" # must be plaintext file\n",
    "output_report = \"{options['output_file']}\"\n",
    "watch = {options['watch_files']}\n",
    "\n",
    "[repro.stage.init]\n",
    "script = \"\"\"\n",
    "{options['repro_init_script']}\n",
    "\"\"\"\n",
    "\n",
    "[repro.stage.develop]\n",
    "script = \"\"\"\n",
    "docker run -v $(pwd):/home/jovyan -p 8888:8888 {options['dev_image_tag']}\n",
    "\\INSERT{{watch_cmd_here}}\n",
    "\"\"\"\n",
    "\n",
    "[repro.stage.build]\n",
    "script = \"\"\"\n",
    "docker run --rm -i -v $(pwd):/home/jovyan -p 8888:8888 {options['dev_image_tag']} python reproduce_work.build() # this replaces instances of INSERTvar in input file\n",
    "docker run --rm -i -v $(pwd):/home -e REPROWORKDIR=\"{options['reproduce_dir']}\" -e REPROWORKOUTFILE=\"{options['output_file']}\" tex-prepare python build.py # this converts the markdown to latex\n",
    "docker run --rm -i --net=none -v $(pwd):/home tex-compile sh -c \"cd /home/{options['reproduce_dir']}/tmp/{options['document_dir']}/latex && xelatex compiled.tex\" # this compiles the latex{options['nbdev_project_cmd']}\n",
    "\"\"\"'''\n",
    "\n",
    "def generate_config(options={}, version=\"reproduce.work/v1/default\"):\n",
    "    inputs = options\n",
    "    verbose_str = 'false'\n",
    "    if inputs=={}:\n",
    "        # Authors section\n",
    "        author1_email = input(\"Enter author's email (required): \")\n",
    "        author1_name = input(\"Enter author's name: \")\n",
    "        author1_affiliation = input(\"Enter author's affiliation: \")\n",
    "        \n",
    "\n",
    "        # Repro stage init section\n",
    "        dev_image_tag = input(\"Enter dev image tag (required; the docker image of your local development workflow): \")\n",
    "        \n",
    "        project_base_url = False\n",
    "        github_repo = input(\"Enter github repo (required): \")\n",
    "\n",
    "        default_repro_settings = (input(\"Set up default reproduce.work environment? (y/n): \") or 'y') == 'y'\n",
    "        if not default_repro_settings:\n",
    "            repro_version = input(f\"Enter reproduce_work API version (Default: {version}): \") or version\n",
    "            nbdev_project = (input(\"Set up default reproduce.work environment? (y/n): \") or 'n') == 'y'\n",
    "            project_full_title = input(\"Enter project full title (options): \") or \"Title goes here\"\n",
    "            project_abstract = input(\"Enter project abstract (optional): \") or \"Abstract goes here.\"\n",
    "            document_dir = input(f\"Enter directory to ensure exists (Default: 'document'): \") or \"document\"\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        author1_email = inputs['authors']['author1']['email']\n",
    "        if 'name' in inputs['authors']['author1']:\n",
    "            author1_name = inputs['authors']['author1']['name']\n",
    "        else:\n",
    "            author1_name = None\n",
    "\n",
    "        if 'affiliation' in inputs['authors']['author1']:\n",
    "            author1_affiliation = inputs['authors']['author1']['affiliation']\n",
    "        else:\n",
    "            author1_affiliation = None\n",
    "            \n",
    "        dev_image_tag = inputs['dev_image_tag']\n",
    "\n",
    "        \n",
    "        github_repo = False\n",
    "        project_base_url = False\n",
    "        project_full_title = 'Title goes here'\n",
    "        project_abstract = 'Abstract goes here.'\n",
    "        if 'project' in inputs:\n",
    "            if 'full_title' in inputs['project']:\n",
    "                project_full_title = inputs['project']['full_title']\n",
    "            if 'abstract' in inputs['project']:\n",
    "                project_abstract = inputs['project']['abstract']\n",
    "            if 'github_repo' in inputs['project']:\n",
    "                github_repo = inputs['project']['github_repo']\n",
    "            if 'project_base_url' in inputs['project']:\n",
    "                project_base_url = inputs['project']['base_url']\n",
    "\n",
    "        if 'repro' in inputs and type(inputs['repro']==str) and inputs['repro'] == 'default':\n",
    "            default_repro_settings = True\n",
    "            nbdev_project = False\n",
    "            repro_version = \"reproduce.work/v1/default\"\n",
    "        \n",
    "        else:\n",
    "            repro_version = \"reproduce.work/v1/default\"\n",
    "            if 'repro' in inputs and 'version' in inputs['repro']:\n",
    "                repro_version = inputs['repro']['version']\n",
    "\n",
    "            nbdev_project = False\n",
    "            if 'nbdev_project' in inputs:\n",
    "                nbdev_project = inputs['nbdev_project']\n",
    "\n",
    "            if 'document_dir' in inputs:\n",
    "                document_dir = inputs['document_dir']\n",
    "            else:\n",
    "                document_dir = 'document'\n",
    "            \n",
    "        verbose = False\n",
    "        if 'verbose' in inputs:\n",
    "            verbose = inputs['verbose']\n",
    "        verbose_str = \"true\" if verbose else \"false\"\n",
    "\n",
    "    if default_repro_settings:\n",
    "        # version reproduce.work/v1/default\n",
    "        document_dir = \"document\"\n",
    "        input_file = f\"{document_dir}/main.md\"\n",
    "        dynamic_file = f\"{reproduce_dir}/pubdata.toml\"\n",
    "        latex_template = f\"{document_dir}/latex/template.tex\"\n",
    "        bibfile =f\"{document_dir}/latex/bibliography.bib\"\n",
    "        output_linefile =f\"{document_dir}/latex/report.tex\"\n",
    "        output_file =f\"{document_dir}/report.pdf\"\n",
    "        watch_files = [input_file, dynamic_file, latex_template, bibfile]\n",
    "\n",
    "    # ensure existence of reproduce_dir and latex subdirectory\n",
    "    \n",
    "    # ensure existence of document_dir and latex subdirectory\n",
    "    Path(document_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(f\"{document_dir}/latex\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ensure existence of main.md\n",
    "    Path(input_file).touch()\n",
    "    \n",
    "    # Check for critical fields\n",
    "    if not dev_image_tag:\n",
    "        printrw(\"Error: Missing required key 'dev_image_tag'.\")\n",
    "        return\n",
    "    \n",
    "    nbdev_project_cmd = \"\"\n",
    "    if nbdev_project:\n",
    "        nbdev_project_cmd = '\\nnbdev_install_hooks && nbdev_export'\n",
    "\n",
    "    github_repo_str = ''\n",
    "    if github_repo:\n",
    "        github_repo_str = f'\\ngithub_repo = \"{github_repo}\"'\n",
    "\n",
    "    if not project_base_url and github_repo:\n",
    "        project_base_url = f\"https://github.com/{github_repo}\"\n",
    "    if project_base_url:\n",
    "        base_url_str = f'\\nbase_url = \"{project_base_url}\"'\n",
    "    else:\n",
    "        base_url_str = ''\n",
    "\n",
    "    #reproduce_dir_default = f\"{reproduce_dir}\"  # this can be changed if needed\n",
    "    #reproduce_dir = input(f\"Enter reproduce directory (Default: {reproduce_dir_default}): \") or reproduce_dir_default\n",
    "    \n",
    "    repro_init_script = f'''docker build -t {dev_image_tag} .\n",
    "docker build -t tex-prepare https://github.com/reproduce-work/tex-prepare.git\n",
    "docker build -t tex-compile https://github.com/reproduce-work/tex-compile.git\n",
    "docker build -t watcher https://github.com/reproduce-work/rwatch.git\n",
    "'''\n",
    "\n",
    "    # ensure reproduce_dir exists\n",
    "    Path(reproduce_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    options = {\n",
    "        'author1_email': author1_email,\n",
    "        'author1_name': author1_name,\n",
    "        'author1_affiliation': author1_affiliation,\n",
    "        'project_full_title': project_full_title,\n",
    "        'project_abstract': project_abstract,\n",
    "        'github_repo_str': github_repo_str,\n",
    "        'base_url_str': base_url_str,\n",
    "        'repro_version': repro_version,\n",
    "        'verbose_str': verbose_str,\n",
    "        'input_file': input_file,\n",
    "        'dynamic_file': dynamic_file,\n",
    "        'latex_template': latex_template,\n",
    "        'bibfile': bibfile,\n",
    "        'watch_files': watch_files,\n",
    "        'repro_init_script': repro_init_script,\n",
    "        'dev_image_tag': dev_image_tag,\n",
    "        'reproduce_dir': reproduce_dir,\n",
    "        'output_linefile': output_linefile,\n",
    "        'output_file': output_file,\n",
    "        'document_dir': document_dir,\n",
    "        'nbdev_project_cmd': nbdev_project_cmd\n",
    "    }\n",
    "\n",
    "    config_str = construct_config(options)\n",
    "\n",
    "    # check for existing config.toml\n",
    "    if os.path.exists(Path(reproduce_dir, 'config.toml')):\n",
    "        # read in existing data\n",
    "        with open(Path(reproduce_dir, 'config.toml'), 'r') as f:\n",
    "            existing_config = toml.load(f)\n",
    "    else:\n",
    "        existing_config = {}\n",
    "\n",
    "    with open(Path(reproduce_dir, 'config.toml'), 'w+') as f:\n",
    "        f.write(config_str)\n",
    "\n",
    "    # By default, TOML files cannot reference variables defined earlier in the file\n",
    "    # This is a hack to get around that which allows me to read in the TOML data,\n",
    "    # and then backfill the watch command with the files listed in the TOML file itself.\n",
    "    if '\\INSERT{watch_cmd_here}' in open(Path(reproduce_dir, 'config.toml'), 'r').read():\n",
    "        with open(Path(reproduce_dir, 'config.toml'), 'r') as f:\n",
    "            base_config_str = f.read()\n",
    "        replaced_config = base_config_str.replace(\n",
    "            '\\INSERT{watch_cmd_here}', \n",
    "            f\"\"\"docker run watcher \"{','.join(watch_files)}\" \"echo \\\"File has changed!\\\" && build_cmd\" \"\"\"\n",
    "        )\n",
    "        with open(Path(reproduce_dir, 'config.toml'), 'w') as f:\n",
    "            f.write(replaced_config)\n",
    "    else:\n",
    "        printrw('did not replace watch command')\n",
    "\n",
    "\n",
    "    with open(Path(reproduce_dir, 'config.toml'), 'r') as f:\n",
    "        config_data = toml.load(f)\n",
    "\n",
    "    # check config_data against existing_config; if any keys overlap, use config_data;\n",
    "    # otherwise keep all unique keys between the two\n",
    "    config_data_tmp = config_data.copy()\n",
    "    config_data = {\n",
    "        **{k:v for k,v in config_data_tmp.items() if k not in existing_config},\n",
    "        **{k:v for k,v in existing_config.items() if k not in config_data_tmp}\n",
    "    }\n",
    "\n",
    "    with open(Path(reproduce_dir, 'config.toml'), 'w') as f:\n",
    "        toml.dump(config_data, f, encoder=ReproduceWorkEncoder())\n",
    "\n",
    "    printrw(f\"Successfully generated reproduce.work configuration at {reproduce_dir}/config.toml\")\n",
    "\n",
    "    # check if input file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        with open(input_file, 'w') as f:\n",
    "            f.write('')\n",
    "        printrw(f\"Successfully generated input file at {input_file}\")\n",
    "\n",
    "    # check if latex template exists\n",
    "    if not os.path.exists(latex_template):\n",
    "        # Fill in with package default\n",
    "        Path(latex_template).parent.mkdir(parents=True, exist_ok=True)\n",
    "        latex_template_default_str = r'\\documentclass[12pt]{article}\\usepackage[english]{babel}\\usepackage{xcolor}\\usepackage[hmargin=1in,vmargin=1in]{geometry}\\usepackage{amsmath}\\usepackage{unicode-math}\\usepackage[round,sort,comma]{natbib}\\bibliographystyle{apa}\\usepackage{setspace}\\usepackage{graphicx}\\usepackage{caption}\\usepackage{subcaption}\\usepackage[colorlinks=true, allcolors=blue]{hyperref}\\usepackage{float}\\usepackage{booktabs}\\usepackage{titlesec}\\newcommand{\\addperiod}[1]{#1.$\\;$}\\titlespacing{\\section}{0pt}{\\parskip}{-\\parskip}\\titleformat{\\subsection}[runin]{\\normalsize\\bfseries}{\\thesubsection}{1em}{\\addperiod}\\titleformat{\\subsubsection}[runin]{\\normalfont\\normalsize\\itshape}{\\thesubsubsection}{1em}{\\addperiod}\\titlespacing{\\subsubsection}{14pt plus 4pt minus 2pt}{0pt}{0pt plus 2pt minus 2pt}\\setlength{\\parindent}{1em}\\makeatletter\\g@addto@macro \\normalsize {\\setlength\\abovedisplayskip{3pt plus 5pt minus 2pt}\\setlength\\belowdisplayskip{3pt plus 5pt minus 2pt}}\\makeatother\\newcommand{\\comment}[1]{}\\begin{document}\\pagenumbering{gobble}\\begin{center}{\\fontsize{16}{16}\\selectfont\\bfseries \\INSERT{config.project.full_title}}\\vspace{5mm}\\begin{table}[!ht]\\begin{center}\\begin{tabular}{c c }\\shortstack{ \\INSERT{config.authors.author1.name} \\\\\\INSERT{config.authors.author1.affiliation} \\\\\\INSERT{config.authors.author1.email} }\\end{tabular}\\end{center}\\end{table}\\vspace{5mm}\\emph{Last updated: \\today}\\vspace{4mm}\\abstract{\\INSERT{config.project.abstract}}\\vspace{2cm}{\\scriptsize \\noindent Notes: }\\vspace{10mm}\\end{center}\\newpage\\doublespacing\\pagenumbering{arabic}\\setcounter{page}{1}%%@@LOWDOWN_CONTENT@@%%\\bibliography{latex/bibliography}\\end{document}'\n",
    "        with open(latex_template, 'w') as f:\n",
    "            f.write(latex_template_default_str)\n",
    "        if config_data['repro']['verbose']:\n",
    "            printrw(f\"╔ω: Successfully generated latex template at {latex_template}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔ω: Notebook nbs/01_core.ipynb already registered in reproduce/config.toml\n",
      "╔ω: Registered notebook https://github.com/reproduce-work/reproduce-work/blob/main/nbs/01_core.ipynb in reproduce/config.toml\n"
     ]
    }
   ],
   "source": [
    "register_notebook('01_core.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔ω: loading embedded link: https://github.com/reproduce-work/reproduce-work/blob/main/reproduce/pubdata.toml#simple_string\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.PublishedObj>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publish_data(\"Hello world!\", \"simple_string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the serialize_to_toml function\n",
    "data_sample = {\n",
    "    'name': 'John',\n",
    "    'age': 28,\n",
    "    'is_student': False,\n",
    "    'scores': [85, 90, 78, 92],\n",
    "    'birthday': pd.Timestamp('2000-01-01'),\n",
    "    'matrix': np.array([[1, 2], [3, 4]]),\n",
    "    'df': pd.DataFrame({\n",
    "        'A': [1, 2, 3],\n",
    "        'B': ['a', 'b', 'c'],\n",
    "        'date': [pd.Timestamp('2022-01-01'), pd.Timestamp('2022-01-02'), pd.Timestamp('2022-01-03')]\n",
    "    }),\n",
    "    'nested_dict': {\n",
    "        'key1': 'value1',\n",
    "        'sub_dict': {\n",
    "            'sub_key': 'sub_value'\n",
    "        }\n",
    "    },\n",
    "    'none_value': None\n",
    "}\n",
    "\n",
    "#publish_data(data_sample, \"data_sample\")  # This should capture this line number and timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>path</th>\n",
       "      <th>start_pos</th>\n",
       "      <th>end_pos</th>\n",
       "      <th>line_range</th>\n",
       "      <th>toml_header</th>\n",
       "      <th>on_line</th>\n",
       "      <th>at_char</th>\n",
       "      <th>hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>published_url</td>\n",
       "      <td>https://github.com/reproduce-work/reproduce-wo...</td>\n",
       "      <td>122</td>\n",
       "      <td>203</td>\n",
       "      <td>(1, 11)</td>\n",
       "      <td>[p_value_str]</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>published_url</td>\n",
       "      <td>https://github.com/reproduce-work/reproduce-wo...</td>\n",
       "      <td>741</td>\n",
       "      <td>822</td>\n",
       "      <td>(13, 24)</td>\n",
       "      <td>[x]</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>published_url</td>\n",
       "      <td>https://github.com/reproduce-work/reproduce-wo...</td>\n",
       "      <td>3620</td>\n",
       "      <td>3701</td>\n",
       "      <td>(26, 37)</td>\n",
       "      <td>[y]</td>\n",
       "      <td>29</td>\n",
       "      <td>18</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>published_url</td>\n",
       "      <td>https://github.com/reproduce-work/reproduce-wo...</td>\n",
       "      <td>6811</td>\n",
       "      <td>6892</td>\n",
       "      <td>(39, 50)</td>\n",
       "      <td>[reproducible_plot_5073a37e]</td>\n",
       "      <td>48</td>\n",
       "      <td>18</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>published_url</td>\n",
       "      <td>https://github.com/reproduce-work/reproduce-wo...</td>\n",
       "      <td>7136</td>\n",
       "      <td>7217</td>\n",
       "      <td>(52, 61)</td>\n",
       "      <td>[simple_string]</td>\n",
       "      <td>53</td>\n",
       "      <td>18</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        variable                                               path  \\\n",
       "0  published_url  https://github.com/reproduce-work/reproduce-wo...   \n",
       "1  published_url  https://github.com/reproduce-work/reproduce-wo...   \n",
       "2  published_url  https://github.com/reproduce-work/reproduce-wo...   \n",
       "3  published_url  https://github.com/reproduce-work/reproduce-wo...   \n",
       "4  published_url  https://github.com/reproduce-work/reproduce-wo...   \n",
       "\n",
       "   start_pos  end_pos line_range                   toml_header  on_line  \\\n",
       "0        122      203    (1, 11)                 [p_value_str]        3   \n",
       "1        741      822   (13, 24)                           [x]       16   \n",
       "2       3620     3701   (26, 37)                           [y]       29   \n",
       "3       6811     6892   (39, 50)  [reproducible_plot_5073a37e]       48   \n",
       "4       7136     7217   (52, 61)               [simple_string]       53   \n",
       "\n",
       "   at_char  hash  \n",
       "0       18  None  \n",
       "1       18  None  \n",
       "2       18  None  \n",
       "3       18  None  \n",
       "4       18  None  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_config = read_base_config()\n",
    "dynamic_loc = Path(base_config['repro']['files']['dynamic'])\n",
    "publinks = find_pubdata_links()\n",
    "pldf = pd.DataFrame(publinks)\n",
    "pldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p_value_str': 'https://github.com/reproduce-work/reproduce-work/blob/main/reproduce/pubdata.toml',\n",
       " 'x': 'https://github.com/reproduce-work/reproduce-work/blob/main/reproduce/pubdata.toml',\n",
       " 'y': 'https://github.com/reproduce-work/reproduce-work/blob/main/reproduce/pubdata.toml',\n",
       " 'reproducible_plot_5073a37e': 'https://github.com/reproduce-work/reproduce-work/blob/main/reproduce/pubdata.toml',\n",
       " 'simple_string': 'https://github.com/reproduce-work/reproduce-work/blob/main/reproduce/pubdata.toml'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars = (pldf.variable=='published_url')\n",
    "dict(zip(\n",
    "    pldf.loc[vars,'toml_header'].str.slice(1,-1).tolist(), \n",
    "    pldf.loc[vars,'path'].tolist()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
