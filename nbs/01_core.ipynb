{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"REPROWORKDIR\"] = \"reproduce\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import functools\n",
    "import hashlib\n",
    "import inspect\n",
    "import re\n",
    "import toml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def printrw(*args, **kwargs):\n",
    "    '''fancy reproduce.work print function'''\n",
    "    # if the first arg is a string, prepend it with ╔ω\n",
    "    if len(args) > 0 and isinstance(args[0], str):\n",
    "        args = (\"╔ω: \"+args[0], *args[1:])\n",
    "\n",
    "    # for each arg, replace newlines with ╚\n",
    "    if len(args) > 0:\n",
    "        new_args = []\n",
    "        for a in args[:-1]:\n",
    "            if isinstance(a, str):  \n",
    "                a = a.replace(\"\\n\", \"\\n║ \")\n",
    "\n",
    "            new_args.append(a)\n",
    "        if isinstance(args[:-1], str): \n",
    "            new_args.append(args[:-1].replace(\"\\n\", \"\\n╚ \"))\n",
    "        else:\n",
    "            new_args.append(args[-1])\n",
    "        args = tuple(new_args)\n",
    "\n",
    "    print(*args, **kwargs, flush=True)\n",
    "\n",
    "    \n",
    "\n",
    "def set_default_dir(quiet=True):\n",
    "    if not os.getenv(\"REPROWORKDIR\", False):\n",
    "        if not quiet:\n",
    "            printrw('Setting reproduce.work config dir to ./reproduce. Be sure to run generate_config() to generate a config file before\\n'+\n",
    "              'executing any other commands. Define `REPROWORKDIR` ENV variable with valid config to avoid this message.')\n",
    "        return Path(\"./reproduce\")\n",
    "    else:\n",
    "        return os.getenv(\"REPROWORKDIR\")\n",
    "\n",
    "reproduce_dir = os.getenv(\"REPROWORKDIR\", set_default_dir(quiet=False))\n",
    "dev_image_tag = os.getenv(\"REPRODEVIMAGE\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_base_config():\n",
    "    with open(Path(reproduce_dir, 'config.toml'), 'r') as f:\n",
    "        base_config = toml.load(f)\n",
    "    return base_config\n",
    "\n",
    "    \n",
    "def toml_dump(val):\n",
    "    # Convert special types to serializable formats\n",
    "    def serialize_special_types(obj):\n",
    "        if isinstance(obj, pd.Timestamp):\n",
    "            return obj.isoformat()\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, pd.DataFrame):\n",
    "            return obj.to_dict(orient='records')\n",
    "        elif obj is None:\n",
    "            return 'None'\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: serialize_special_types(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [serialize_special_types(i) for i in obj]\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    serialized_val = serialize_special_types(val)\n",
    "    \n",
    "    return toml.loads(toml.dumps({'val': serialized_val}))['val']\n",
    "\n",
    "class ReproduceWorkEncoder(toml.TomlEncoder):\n",
    "    def dump_str(self, v):\n",
    "        \"\"\"Encode a string.\"\"\"\n",
    "        if \"\\n\" in v:\n",
    "            return v  # If it's a multi-line string, return it as-is\n",
    "        return super().dump_str(v)\n",
    "    \n",
    "    def dump_value(self, v):\n",
    "        \"\"\"Determine the type of a Python object and serialize it accordingly.\"\"\"\n",
    "        if isinstance(v, str) and \"\\n\" in v:\n",
    "            return '\"\"\"\\n' + v.strip() + '\\n' + '\"\"\"'\n",
    "        return super().dump_value(v)\n",
    "\n",
    "\n",
    "def validate_base_config(base_config, quiet=False):\n",
    "    required_keys = ['authors', 'repro']\n",
    "    for key in required_keys:\n",
    "        if key not in base_config:\n",
    "            #printrw(toml.dumps(base_config))\n",
    "            if not quiet:\n",
    "                printrw(f\"Error with ╔ω config: Missing required field '{key}' in config.toml\")\n",
    "            return False\n",
    "        if key=='repro':\n",
    "            if 'stages' not in base_config['repro']:\n",
    "                if not quiet:\n",
    "                    printrw(f\"Error with ╔ω config:: Missing required field 'repro.stages' in reproduce.work configuration at {reproduce_dir}/config.toml\")\n",
    "                return False\n",
    "            for stage in base_config['repro']['stages']:\n",
    "                if (f'repro.stage.{stage}' not in base_config) and (stage not in base_config['repro']['stage']):\n",
    "                    if not quiet:\n",
    "                        (toml.dumps(base_config, encoder=ReproduceWorkEncoder()))\n",
    "                    printrw(f\"Error with ╔ω config:: Missing required field repro.stage.{stage} in reproduce.work configuration at {reproduce_dir}/config.toml\")\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "def requires_config(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            config = read_base_config()\n",
    "        except:\n",
    "            raise Exception(\"Your reproduce.work config is either missing or invalid. Run generate_config() to generate a config file.\")\n",
    "        if not validate_base_config(config):\n",
    "            raise Exception(\"Your reproduce.work configuration is not valid.\")\n",
    "        if func.__name__ in [\"publish_data\",\"publish_file\"] and VAR_REGISTRY['REPROWORK_REMOTE_URL'] is None:\n",
    "            raise Exception(f\"register_notebook(*) to use this function: {func.__name__}\")\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "VAR_REGISTRY = {\n",
    "    'REPROWORK_REMOTE_URL': None,\n",
    "    'REPROWORK_ACTIVE_NOTEBOOK': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "import toml\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "class ReproduceWorkEncoder(toml.TomlEncoder):\n",
    "    def dump_str(self, v):\n",
    "        \"\"\"Encode a string.\"\"\"\n",
    "        if \"\\n\" in v:\n",
    "            return v  # If it's a multi-line string, return it as-is\n",
    "        return super().dump_str(v)\n",
    "    \n",
    "    def dump_value(self, v):\n",
    "        \"\"\"Determine the type of a Python object and serialize it accordingly.\"\"\"\n",
    "        if isinstance(v, str) and \"\\n\" in v:\n",
    "            return '\"\"\"\\n' + v.strip() + '\\n' + '\"\"\"'\n",
    "        return super().dump_value(v)\n",
    "\n",
    "def construct_config(options):\n",
    "\n",
    "    for key in ['author1_email', 'author1_name', 'author1_affiliation',\n",
    "                'project_full_title', 'project_abstract', 'github_repo_str', \n",
    "                'base_url_str', 'repro_version', 'verbose_str', 'input_file',\n",
    "                'dynamic_file', 'bibfile', 'latex_template', 'watch_files', \n",
    "                'repro_init_script', 'dev_image_tag', 'reproduce_dir',\n",
    "                'output_file', 'output_linefile', 'document_dir', 'nbdev_project_cmd']:\n",
    "        \n",
    "        if key not in options:\n",
    "            printrw(f\"Error: Missing required key '{key}' in options.\")\n",
    "            return False\n",
    "        \n",
    "    newline = \"\\n\"\n",
    "    return f'''\n",
    "[authors]\n",
    "author1.email = \"{options['author1_email']}\"{newline + 'author1.name = \"' + options['author1_name'] + '\"' if options['author1_name'] else \"\"}{newline + 'author1.affiliation = \"' + options['author1_affiliation'] + '\"' if options['author1_affiliation'] else \"\"}\n",
    "\n",
    "[project]\n",
    "full_title = \"{options['project_full_title']}\"\n",
    "abstract = \"\"\"\n",
    "{options['project_abstract']}\n",
    "\"\"\"{options['github_repo_str']}{options['base_url_str']}\n",
    "\n",
    "# reproduce.work configuration\n",
    "[repro]\n",
    "version = \"{options['repro_version']}\"\n",
    "stages = [\"init\", \"develop\", \"build\"]\n",
    "verbose = {options['verbose_str']}\n",
    "document_dir = \"{options['document_dir']}\"\n",
    "\n",
    "[repro.files]\n",
    "input = \"{options['input_file']}\"\n",
    "dynamic = \"{options['dynamic_file']}\"\n",
    "latex_template = \"{options['latex_template']}\"\n",
    "bibfile = \"{options['bibfile']}\"\n",
    "output_linefile = \"{options['output_linefile']}\" # must be plaintext file\n",
    "output_report = \"{options['output_file']}\"\n",
    "watch = {options['watch_files']}\n",
    "\n",
    "[repro.stage.init]\n",
    "script = \"\"\"\n",
    "{options['repro_init_script']}\n",
    "\"\"\"\n",
    "\n",
    "[repro.stage.develop]\n",
    "script = \"\"\"\n",
    "docker run -v $(pwd):/home/jovyan -p 8888:8888 {options['dev_image_tag']}\n",
    "\\INSERT{{watch_cmd_here}}\n",
    "\"\"\"\n",
    "\n",
    "[repro.stage.build]\n",
    "script = \"\"\"\n",
    "docker run --rm -i -v $(pwd):/home/jovyan -p 8888:8888 {options['dev_image_tag']} python reproduce_work.build() # this replaces instances of INSERTvar in input file\n",
    "docker run --rm -i -v $(pwd):/home -e REPROWORKDIR=\"{options['reproduce_dir']}\" -e REPROWORKOUTFILE=\"{options['output_file']}\" tex-prepare python build.py # this converts the markdown to latex\n",
    "docker run --rm -i --net=none -v $(pwd):/home tex-compile sh -c \"cd /home/{options['reproduce_dir']}/tmp/{options['document_dir']}/latex && xelatex compiled.tex\" # this compiles the latex{options['nbdev_project_cmd']}\n",
    "\"\"\"'''\n",
    "\n",
    "def generate_config(options={}, version=\"reproduce.work/v1/default\"):\n",
    "    inputs = options\n",
    "    verbose_str = 'false'\n",
    "    if inputs=={}:\n",
    "        # Authors section\n",
    "        author1_email = input(\"Enter author's email (required): \")\n",
    "        author1_name = input(\"Enter author's name: \")\n",
    "        author1_affiliation = input(\"Enter author's affiliation: \")\n",
    "        \n",
    "\n",
    "        # Repro stage init section\n",
    "        dev_image_tag = input(\"Enter dev image tag (required; the docker image of your local development workflow): \")\n",
    "        \n",
    "        project_base_url = False\n",
    "        github_repo = input(\"Enter github repo (required): \")\n",
    "\n",
    "        default_repro_settings = (input(\"Set up default reproduce.work environment? (y/n): \") or 'y') == 'y'\n",
    "        if not default_repro_settings:\n",
    "            repro_version = input(f\"Enter reproduce_work API version (Default: {version}): \") or version\n",
    "            nbdev_project = (input(\"Set up default reproduce.work environment? (y/n): \") or 'n') == 'y'\n",
    "            project_full_title = input(\"Enter project full title (options): \") or \"Title goes here\"\n",
    "            project_abstract = input(\"Enter project abstract (optional): \") or \"Abstract goes here.\"\n",
    "            document_dir = input(f\"Enter directory to ensure exists (Default: 'document'): \") or \"document\"\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        author1_email = inputs['authors']['author1']['email']\n",
    "        if 'name' in inputs['authors']['author1']:\n",
    "            author1_name = inputs['authors']['author1']['name']\n",
    "        else:\n",
    "            author1_name = None\n",
    "\n",
    "        if 'affiliation' in inputs['authors']['author1']:\n",
    "            author1_affiliation = inputs['authors']['author1']['affiliation']\n",
    "        else:\n",
    "            author1_affiliation = None\n",
    "            \n",
    "        dev_image_tag = inputs['dev_image_tag']\n",
    "\n",
    "        \n",
    "        github_repo = False\n",
    "        project_base_url = False\n",
    "        project_full_title = 'Title goes here'\n",
    "        project_abstract = 'Abstract goes here.'\n",
    "        if 'project' in inputs:\n",
    "            if 'full_title' in inputs['project']:\n",
    "                project_full_title = inputs['project']['full_title']\n",
    "            if 'abstract' in inputs['project']:\n",
    "                project_abstract = inputs['project']['abstract']\n",
    "            if 'github_repo' in inputs['project']:\n",
    "                github_repo = inputs['project']['github_repo']\n",
    "            if 'project_base_url' in inputs['project']:\n",
    "                project_base_url = inputs['project']['base_url']\n",
    "\n",
    "        if 'repro' in inputs and type(inputs['repro']==str) and inputs['repro'] == 'default':\n",
    "            default_repro_settings = True\n",
    "            nbdev_project = False\n",
    "            repro_version = \"reproduce.work/v1/default\"\n",
    "        \n",
    "        else:\n",
    "            repro_version = \"reproduce.work/v1/default\"\n",
    "            if 'repro' in inputs and 'version' in inputs['repro']:\n",
    "                repro_version = inputs['repro']['version']\n",
    "\n",
    "            nbdev_project = False\n",
    "            if 'nbdev_project' in inputs:\n",
    "                nbdev_project = inputs['nbdev_project']\n",
    "\n",
    "            if 'document_dir' in inputs:\n",
    "                document_dir = inputs['document_dir']\n",
    "            else:\n",
    "                document_dir = 'document'\n",
    "            \n",
    "        verbose = False\n",
    "        if 'verbose' in inputs:\n",
    "            verbose = inputs['verbose']\n",
    "        verbose_str = \"true\" if verbose else \"false\"\n",
    "\n",
    "    if default_repro_settings:\n",
    "        # version reproduce.work/v1/default\n",
    "        document_dir = \"document\"\n",
    "        input_file = f\"{document_dir}/main.md\"\n",
    "        dynamic_file = f\"{reproduce_dir}/pubdata.toml\"\n",
    "        latex_template = f\"{document_dir}/latex/template.tex\"\n",
    "        bibfile =f\"{document_dir}/latex/bibliography.bib\"\n",
    "        output_linefile =f\"{document_dir}/latex/report.tex\"\n",
    "        output_file =f\"{document_dir}/report.pdf\"\n",
    "        watch_files = [input_file, dynamic_file, latex_template, bibfile]\n",
    "\n",
    "    # ensure existence of reproduce_dir and latex subdirectory\n",
    "    \n",
    "    # ensure existence of document_dir and latex subdirectory\n",
    "    Path(document_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(f\"{document_dir}/latex\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ensure existence of main.md\n",
    "    Path(input_file).touch()\n",
    "    \n",
    "    # Check for critical fields\n",
    "    if not dev_image_tag:\n",
    "        printrw(\"Error: Missing required key 'dev_image_tag'.\")\n",
    "        return\n",
    "    \n",
    "    nbdev_project_cmd = \"\"\n",
    "    if nbdev_project:\n",
    "        nbdev_project_cmd = '\\nnbdev_install_hooks && nbdev_export'\n",
    "\n",
    "    github_repo_str = ''\n",
    "    if github_repo:\n",
    "        github_repo_str = f'\\ngithub_repo = \"{github_repo}\"'\n",
    "\n",
    "    if not project_base_url and github_repo:\n",
    "        project_base_url = f\"https://github.com{github_repo}\"\n",
    "    if project_base_url:\n",
    "        base_url_str = f'\\nbase_url = \"{project_base_url}\"'\n",
    "    else:\n",
    "        base_url_str = ''\n",
    "\n",
    "    #reproduce_dir_default = f\"{reproduce_dir}\"  # this can be changed if needed\n",
    "    #reproduce_dir = input(f\"Enter reproduce directory (Default: {reproduce_dir_default}): \") or reproduce_dir_default\n",
    "    \n",
    "    repro_init_script = f'''docker build -t {dev_image_tag} .\n",
    "docker build -t tex-prepare https://github.com/reproduce-work/tex-prepare.git\n",
    "docker build -t tex-compile https://github.com/reproduce-work/tex-compile.git\n",
    "docker build -t watcher https://github.com/reproduce-work/rwatch.git\n",
    "'''\n",
    "\n",
    "    # ensure reproduce_dir exists\n",
    "    Path(reproduce_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    options = {\n",
    "        'author1_email': author1_email,\n",
    "        'author1_name': author1_name,\n",
    "        'author1_affiliation': author1_affiliation,\n",
    "        'project_full_title': project_full_title,\n",
    "        'project_abstract': project_abstract,\n",
    "        'github_repo_str': github_repo_str,\n",
    "        'base_url_str': base_url_str,\n",
    "        'repro_version': repro_version,\n",
    "        'verbose_str': verbose_str,\n",
    "        'input_file': input_file,\n",
    "        'dynamic_file': dynamic_file,\n",
    "        'latex_template': latex_template,\n",
    "        'bibfile': bibfile,\n",
    "        'watch_files': watch_files,\n",
    "        'repro_init_script': repro_init_script,\n",
    "        'dev_image_tag': dev_image_tag,\n",
    "        'reproduce_dir': reproduce_dir,\n",
    "        'output_linefile': output_linefile,\n",
    "        'output_file': output_file,\n",
    "        'document_dir': document_dir,\n",
    "        'nbdev_project_cmd': nbdev_project_cmd\n",
    "    }\n",
    "\n",
    "    config_str = construct_config(options)\n",
    "\n",
    "    # check for existing config.toml\n",
    "    if os.path.exists(Path(reproduce_dir, 'config.toml')):\n",
    "        # read in existing data\n",
    "        with open(Path(reproduce_dir, 'config.toml'), 'r') as f:\n",
    "            existing_config = toml.load(f)\n",
    "    else:\n",
    "        existing_config = {}\n",
    "\n",
    "    with open(Path(reproduce_dir, 'config.toml'), 'w+') as f:\n",
    "        f.write(config_str)\n",
    "\n",
    "    # By default, TOML files cannot reference variables defined earlier in the file\n",
    "    # This is a hack to get around that which allows me to read in the TOML data,\n",
    "    # and then backfill the watch command with the files listed in the TOML file itself.\n",
    "    if '\\INSERT{watch_cmd_here}' in open(Path(reproduce_dir, 'config.toml'), 'r').read():\n",
    "        with open(Path(reproduce_dir, 'config.toml'), 'r') as f:\n",
    "            base_config_str = f.read()\n",
    "        replaced_config = base_config_str.replace(\n",
    "            '\\INSERT{watch_cmd_here}', \n",
    "            f\"\"\"docker run watcher \"{','.join(watch_files)}\" \"echo \\\"File has changed!\\\" && build_cmd\" \"\"\"\n",
    "        )\n",
    "        with open(Path(reproduce_dir, 'config.toml'), 'w') as f:\n",
    "            f.write(replaced_config)\n",
    "    else:\n",
    "        printrw('did not replace watch command')\n",
    "\n",
    "\n",
    "    with open(Path(reproduce_dir, 'config.toml'), 'r') as f:\n",
    "        config_data = toml.load(f)\n",
    "\n",
    "    # check config_data against existing_config; if any keys overlap, use config_data;\n",
    "    # otherwise keep all unique keys between the two\n",
    "    config_data_tmp = config_data.copy()\n",
    "    config_data = {\n",
    "        **{k:v for k,v in config_data_tmp.items() if k not in existing_config},\n",
    "        **{k:v for k,v in existing_config.items() if k not in config_data_tmp}\n",
    "    }\n",
    "\n",
    "    with open(Path(reproduce_dir, 'config.toml'), 'w') as f:\n",
    "        toml.dump(config_data, f, encoder=ReproduceWorkEncoder())\n",
    "\n",
    "    printrw(f\"Successfully generated reproduce.work configuration at {reproduce_dir}/config.toml\")\n",
    "\n",
    "    # check if input file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        with open(input_file, 'w') as f:\n",
    "            f.write('')\n",
    "        printrw(f\"Successfully generated input file at {input_file}\")\n",
    "\n",
    "    # check if latex template exists\n",
    "    if not os.path.exists(latex_template):\n",
    "        # Fill in with package default\n",
    "        Path(latex_template).parent.mkdir(parents=True, exist_ok=True)\n",
    "        latex_template_default_str = r'\\documentclass[12pt]{article}\\usepackage[english]{babel}\\usepackage{xcolor}\\usepackage[hmargin=1in,vmargin=1in]{geometry}\\usepackage{amsmath}\\usepackage{unicode-math}\\usepackage[round,sort,comma]{natbib}\\bibliographystyle{apa}\\usepackage{setspace}\\usepackage{graphicx}\\usepackage{caption}\\usepackage{subcaption}\\usepackage[colorlinks=true, allcolors=blue]{hyperref}\\usepackage{float}\\usepackage{booktabs}\\usepackage{titlesec}\\newcommand{\\addperiod}[1]{#1.$\\;$}\\titlespacing{\\section}{0pt}{\\parskip}{-\\parskip}\\titleformat{\\subsection}[runin]{\\normalsize\\bfseries}{\\thesubsection}{1em}{\\addperiod}\\titleformat{\\subsubsection}[runin]{\\normalfont\\normalsize\\itshape}{\\thesubsubsection}{1em}{\\addperiod}\\titlespacing{\\subsubsection}{14pt plus 4pt minus 2pt}{0pt}{0pt plus 2pt minus 2pt}\\setlength{\\parindent}{1em}\\makeatletter\\g@addto@macro \\normalsize {\\setlength\\abovedisplayskip{3pt plus 5pt minus 2pt}\\setlength\\belowdisplayskip{3pt plus 5pt minus 2pt}}\\makeatother\\newcommand{\\comment}[1]{}\\begin{document}\\pagenumbering{gobble}\\begin{center}{\\fontsize{16}{16}\\selectfont\\bfseries \\INSERT{config.project.full_title}}\\vspace{5mm}\\begin{table}[!ht]\\begin{center}\\begin{tabular}{c c }\\shortstack{ \\INSERT{config.authors.author1.name} \\\\\\INSERT{config.authors.author1.affiliation} \\\\\\INSERT{config.authors.author1.email} }\\end{tabular}\\end{center}\\end{table}\\vspace{5mm}\\emph{Last updated: \\today}\\vspace{4mm}\\abstract{\\INSERT{config.project.abstract}}\\vspace{2cm}{\\scriptsize \\noindent Notes: }\\vspace{10mm}\\end{center}\\newpage\\doublespacing\\pagenumbering{arabic}\\setcounter{page}{1}%%@@LOWDOWN_CONTENT@@%%\\bibliography{latex/bibliography}\\end{document}'\n",
    "        with open(latex_template, 'w') as f:\n",
    "            f.write(latex_template_default_str)\n",
    "        if config_data['repro']['verbose']:\n",
    "            printrw(f\"╔ω: Successfully generated latex template at {latex_template}\")\n",
    "\n",
    "\n",
    "# Write some basic tests\n",
    "def test_validate_base_config():\n",
    "    # Test a valid base_config\n",
    "    base_config = {\n",
    "        'authors': {\n",
    "            'author1.name': 'Alex P. Miller',\n",
    "            'author1.affiliation': 'USC Marshall School of Business',\n",
    "            'author1.email': 'alex.miller@marshall.usc.edu'\n",
    "        },\n",
    "        'repro': {\n",
    "            'version': 'reproduce.work/v1/default',\n",
    "            'stages': ['init', 'develop', 'build']\n",
    "        },\n",
    "        'repro.files': {\n",
    "            'input': 'document/main.md',\n",
    "            'dynamic': 'document/pubdata.toml',\n",
    "            'latex_template': 'document/latex/template.tex',\n",
    "            'output_linefile': 'document/latex/compiled.tex',\n",
    "            'output_file': 'document/latex/compiled.pdf',\n",
    "            'watch': ['document/main.md', 'document/pubdata.toml', 'document/latex/template.tex']\n",
    "        },\n",
    "        'repro.stage.init': {\n",
    "            'script': 'docker build -t {dev_image_tag} .\\ndocker build -t tinytex {reproduce_dir}/Dockerfile.tinytex\\ndocker build -t watcher {reproduce_dir}/Dockerfile.watch\\n'\n",
    "        },\n",
    "        'repro.stage.develop': {\n",
    "            'script': 'docker run -v $(pwd):/home/jovyan -p 8888:8888 {dev_image_tag} start.sh jupyter lab --LabApp.token=\\'\\'\\n\\\\INSERT{watch_cmd_here}\\n'\n",
    "        },\n",
    "        'repro.stage.build': {\n",
    "            'script': 'docker run --rm -i -v $(pwd):/home/jovyan -p 8888:8888 {dev_image_tag} python reproduce_work.build() # this replaces instances of \\\\INSERT{var} in `input` file\\ndocker run --rm -i -v $(pwd):/home lowdown # this converts the markdown to latex\\ndocker run --rm -i --net=none -v $(pwd):/home tinytex sh -c \"cd /home/document/latex && xelatex compiled.tex\" # this compiles the latex\\n'\n",
    "        }\n",
    "    }\n",
    "    assert validate_base_config(base_config) == True\n",
    "\n",
    "# Test an invalid base_config\n",
    "base_config = {\n",
    "    'auhors': {\n",
    "        'author1.name': 'Alex P. Miller',\n",
    "        'author1.affiliation': 'USC Marshall School of Business',\n",
    "        'author1.email': 'alex.miller@marshall.usc.edu'\n",
    "    },\n",
    "    'repro': {\n",
    "        'version': 'reproduce.work/v1/default',\n",
    "        'stages': ['init', 'develop', 'build']\n",
    "    },\n",
    "    'repro.files': {\n",
    "        'input': 'document/main.md',\n",
    "        'dynamic': 'document/pubdata.toml',\n",
    "        'latex_template': 'document/latex/template.tex',\n",
    "        'output': 'document/latex/compiled.tex'\n",
    "    },\n",
    "    'repro.stage.init': {\n",
    "        'script': 'docker build -t {dev_image_tag} .\\ndocker build -t tinytex {reproduce_dir}/Dockerfile.tinytex\\ndocker build -t watcher {reproduce_dir}/Dockerfile.watch\\n'\n",
    "    },\n",
    "    'repro.stage.develop': {\n",
    "        'script': 'docker run -v $(pwd):/home/jovyan -p 8888:8888 {dev_image_tag} start.sh jupyter lab --LabApp.token=\\'\\'\\n\\\\INSERT{watch_cmd_here}\\n'\n",
    "    },\n",
    "    'repro.stage.build': {\n",
    "        'script': 'docker run --rm -i -v $(pwd):/home/jovyan -p 8888:8888 {dev_image_tag} python reproduce_work.build() # this replaces instances of \\\\INSERT{var} in `input` file\\ndocker run --rm -i -v $(pwd):/home lowdown # this converts the markdown to latex\\ndocker run --rm -i --net=none -v $(pwd):/home tinytex sh -c \"cd /home/document/latex && xelatex compiled.tex\" # this compiles the latex\\n'\n",
    "    }\n",
    "}\n",
    "assert validate_base_config(base_config, quiet=True) == False\n",
    "\n",
    "#printrw(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_config = {\n",
    "        'authors': {\n",
    "            'author1.name': 'Alex P. Miller',\n",
    "            'author1.affiliation': 'USC Marshall School of Business',\n",
    "            'author1.email': 'alex.miller@marshall.usc.edu'\n",
    "        },\n",
    "        'repro': {\n",
    "            'version': 'reproduce.work/v1/default',\n",
    "            'stages': ['init', 'develop', 'build']\n",
    "        },\n",
    "        'repro.files': {\n",
    "            'input': 'document/main.md',\n",
    "            'dynamic': 'document/pubdata.toml',\n",
    "            'latex_template': 'document/latex/template.tex',\n",
    "            'output': 'document/latex/compiled.tex'\n",
    "        },\n",
    "        'repro.stage.init': {\n",
    "            'script': 'docker build -t {dev_image_tag} .\\ndocker build -t tinytex {reproduce_dir}/Dockerfile.tinytex\\ndocker build -t watcher {reproduce_dir}/Dockerfile.watch\\n'\n",
    "        },\n",
    "        'repro.stage.develop': {\n",
    "            'script': 'docker run -v $(pwd):/home/jovyan -p 8888:8888 {dev_image_tag} start.sh jupyter lab --LabApp.token=\\'\\'\\n\\\\INSERT{watch_cmd_here}\\n'\n",
    "        },\n",
    "        'repro.stage.build': {\n",
    "            'script': 'docker run --rm -i -v $(pwd):/home/jovyan -p 8888:8888 {dev_image_tag} python reproduce_work.build() # this replaces instances of \\\\INSERT{var} in `input` file\\ndocker run --rm -i -v $(pwd):/home lowdown # this converts the markdown to latex\\ndocker run --rm -i --net=none -v $(pwd):/home tinytex sh -c \"cd /home/document/latex && xelatex compiled.tex\" # this compiles the latex\\n'\n",
    "        }\n",
    "    }\n",
    "validate_base_config(base_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import functools\n",
    "\n",
    "def update_watched_files(add=[], remove=[], quiet=False):\n",
    "    base_config = read_base_config()\n",
    "    existing_files = base_config['repro']['files']['watch']\n",
    "    new_files = existing_files + [a for a in add if a not in existing_files]\n",
    "    new_files = [f for f in new_files if f not in remove]\n",
    "    base_config['repro']['files']['watch'] = new_files\n",
    "\n",
    "    current_develop_script = base_config['repro']['stage']['develop']['script']\n",
    "    \n",
    "    # regex to replace content in string matching 'watcher \\\"{to_replace}\\\"'\n",
    "    # with 'watcher \\\"{new_files}\\\"'\n",
    "    # and replace 'build_cmd' with 'python reproduce_work.build()'\n",
    "    import re\n",
    "    new_develop_script = re.sub(\n",
    "        r'watcher \\\"(.*?)\\\"', \n",
    "        f'watcher \\\"{\",\".join([f.strip() for f in new_files])}\\\"'.strip().rstrip(\",\"), \n",
    "        current_develop_script\n",
    "    )\n",
    "    base_config['repro']['stage']['develop']['script'] = new_develop_script\n",
    "\n",
    "    if current_develop_script!=new_develop_script:\n",
    "        with open(Path(reproduce_dir, 'config.toml'), 'w') as f:\n",
    "            toml.dump(base_config, f, encoder=ReproduceWorkEncoder())\n",
    "            \n",
    "        if base_config['repro']['verbose'] and not quiet:\n",
    "            printrw(f\"Updated watched files to {new_files}\")\n",
    "\n",
    "    return new_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def reproducible_old(var_assignment_func):\n",
    "    \"\"\"\n",
    "    A decorator to register the line number and timestamp when a variable is assigned.\n",
    "    \"\"\"\n",
    "    @functools.wraps(var_assignment_func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Extract value and var_name from args\n",
    "        # Assumes the decorated function always takes at least two arguments: value and var_name\n",
    "        value, var_name = args[0], args[1]\n",
    "\n",
    "        # Extract metadata from kwargs or default to an empty dictionary\n",
    "        metadata = kwargs.get('metadata', {})\n",
    "\n",
    "        # Get the current frame and line number\n",
    "        frame = inspect.currentframe()\n",
    "        line_number = frame.f_back.f_lineno\n",
    "\n",
    "        # Get the current timestamp\n",
    "        timestamp = datetime.datetime.now().isoformat()\n",
    "\n",
    "        # Get the filename of the caller\n",
    "        filename = frame.f_back.f_code.co_filename\n",
    "\n",
    "        # Execute the variable assignment function\n",
    "        result = var_assignment_func(*args, **kwargs)\n",
    "\n",
    "        # Register the variable name, line number, timestamp, and filename\n",
    "        VAR_REGISTRY[var_name] = {\n",
    "            \"type\": \"string\",\n",
    "            \"timestamp\": timestamp,\n",
    "        }\n",
    "\n",
    "        if type(value) is not str:\n",
    "            value = str(value)\n",
    "            printrw(f\"WARNING: value of {var_name} was not a string. Converted to string: {value}.\")\n",
    "\n",
    "        VAR_REGISTRY[var_name]['value'] = value\n",
    "\n",
    "        metadata.update(VAR_REGISTRY[var_name])\n",
    "\n",
    "        config = read_base_config()\n",
    "\n",
    "        # check if dynamic file exists\n",
    "        if not os.path.exists(Path(config['repro']['files']['dynamic'])):\n",
    "            with open(Path(config['repro']['files']['dynamic']), 'w') as file:\n",
    "                file.write(toml.dumps({}))\n",
    "        with open(Path(config['repro']['files']['dynamic']), 'r') as file:\n",
    "            dynamic_data = toml.load(file)\n",
    "\n",
    "        dynamic_data[var_name] = metadata\n",
    "\n",
    "        with open(Path(config['repro']['files']['dynamic']), 'w') as file:\n",
    "            toml.dump(dynamic_data, file, encoder=ReproduceWorkEncoder())\n",
    "\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "#@reproducible\n",
    "#def publish_variable(value, var_name, metadata={}):\n",
    "#    globals()[var_name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "def get_cell_index():\n",
    "    \"\"\"\n",
    "    Get the current cell index in a Jupyter notebook environment.\n",
    "    If not in Jupyter, return None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Execute JavaScript to get the current cell index\n",
    "        get_ipython().run_cell_magic('javascript', '', 'IPython.notebook.kernel.execute(\\'current_cell_index = \\' + IPython.notebook.get_selected_index())')\n",
    "        return current_cell_index\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def check_for_defintion_in_context(function_name='save'):\n",
    "    assert function_name in ['save', 'assign'], \"function_name must be either 'save' or 'assign'\"\n",
    "    \n",
    "    from IPython import get_ipython\n",
    "    ip = get_ipython()\n",
    "\n",
    "    # Check if in Jupyter environment\n",
    "    if ip is None:\n",
    "        \n",
    "        #fill this in \n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        # Get the input history\n",
    "        #lineno = inspect.stack()[0].lineno\n",
    "        raw_hist = ip.history_manager.input_hist_raw\n",
    "        current_cell = raw_hist[-1]\n",
    "\n",
    "\n",
    "        matches = re.findall(rf\"{function_name}\\((.+?),\", current_cell)\n",
    "                \n",
    "        if matches:\n",
    "            # save call\n",
    "            defined_var = matches[0].strip()\n",
    "            definition_cell_content = ''\n",
    "            \n",
    "            for prior_cell in raw_hist[-2::-1]:\n",
    "                #printrw(prior_cell)\n",
    "                if f'{defined_var} =' in prior_cell or f'{defined_var}=' in prior_cell:\n",
    "                    definition_cell_content = prior_cell\n",
    "                    break\n",
    "            \n",
    "            # find the line number of the where the variable was defined\n",
    "            # Give a window of 5 lines around the definition call\n",
    "            def_cell_lines = definition_cell_content.split('\\n')\n",
    "            if len(def_cell_lines)>0:\n",
    "                lineno = None\n",
    "                for line_num, line in enumerate(def_cell_lines):\n",
    "                    if defined_var in line:\n",
    "                        lineno = line_num\n",
    "                        break\n",
    "                if lineno:\n",
    "                    definition_context = (\n",
    "                        '\\n'.join(def_cell_lines[max(0, lineno-5):lineno]) + \n",
    "                        '\\nFLAG' + def_cell_lines[lineno] + '\\n' +\n",
    "                        '\\n'.join(def_cell_lines[lineno+1:min(len(def_cell_lines), lineno+5)])\n",
    "                    )\n",
    "                else:\n",
    "                    definition_context = None\n",
    "\n",
    "            else:\n",
    "                definition_context = None\n",
    "\n",
    "            \n",
    "            save_cell_lines = current_cell.split('\\n')\n",
    "            if len(save_cell_lines)>0:\n",
    "                save_lineno = None\n",
    "                for line_num, line in enumerate(save_cell_lines):\n",
    "                    if 'save(' in line:\n",
    "                        save_lineno = line_num\n",
    "                        break\n",
    "                \n",
    "                if save_lineno:\n",
    "                    save_context = (\n",
    "                        '\\n'.join(save_cell_lines[max(0, save_lineno-5):save_lineno]) + \n",
    "                        '\\nFLAG' + save_cell_lines[save_lineno] + '\\n' +\n",
    "                        '\\n'.join(save_cell_lines[save_lineno+1:min(len(save_cell_lines), save_lineno+5)])\n",
    "                    )\n",
    "                else:\n",
    "                    save_context = None\n",
    "                \n",
    "            else:\n",
    "                save_context = None\n",
    "            \n",
    "\n",
    "        else:\n",
    "            # not a save call\n",
    "            save_context = None\n",
    "            definition_context = None\n",
    "\n",
    "        return(save_context, definition_context)\n",
    "\n",
    "\n",
    "\n",
    "def serialize_to_toml(data, root=True):\n",
    "    \"\"\"Unified function to serialize various Python data types to TOML format.\"\"\"\n",
    "    toml_string = \"\"\n",
    "    \n",
    "    # Handle numpy array\n",
    "    if isinstance(data, np.ndarray):\n",
    "        toml_string += f\"array = {data.tolist()}\"\n",
    "    \n",
    "    # Handle pandas DataFrame\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        toml_string += \"[dataframe]\\n\"\n",
    "        for col in data.columns:\n",
    "            values = data[col].tolist()\n",
    "            if all(isinstance(val, (int, float)) for val in values):\n",
    "                toml_string += f\"{col} = {values}\\n\"\n",
    "            else:\n",
    "                values_str = ['\"' + str(val) + '\"' for val in values]\n",
    "                toml_string += f\"{col} = [{', '.join(values_str)}]\\n\"\n",
    "        return toml_string\n",
    "    \n",
    "    # Handle dictionary\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, str):\n",
    "                toml_string += f\"{key} = \\\"{value}\\\"\\n\"\n",
    "            elif isinstance(value, (int, float)):\n",
    "                toml_string += f\"{key} = {value}\\n\"\n",
    "            elif isinstance(value, bool):\n",
    "                toml_string += f\"{key} = {str(value).lower()}\\n\"\n",
    "            elif isinstance(value, (list, set, tuple)):\n",
    "                values = \", \".join([str(v) for v in value])\n",
    "                toml_string += f\"{key} = [{values}]\\n\"\n",
    "            elif value is None:\n",
    "                toml_string += f\"{key} = null\\n\"\n",
    "            elif isinstance(value, (np.datetime64, pd.Timestamp)):\n",
    "                toml_string += f\"{key} = \\\"{str(value)}\\\"\\n\"\n",
    "            elif isinstance(value, dict) or isinstance(value, pd.DataFrame):\n",
    "                # Recursive call for nested dictionaries or DataFrames\n",
    "                nested_str = serialize_to_toml(value, root=False)\n",
    "                toml_string += f\"[{key}]\\n{nested_str}\\n\"\n",
    "    \n",
    "    # If it's the root call, remove any trailing newline\n",
    "    if root:\n",
    "        toml_string = toml_string.rstrip()\n",
    "    return toml_string\n",
    "\n",
    "\n",
    "class ReproduceWorkEncoder(toml.TomlEncoder):\n",
    "    def dump_str(self, v):\n",
    "        \"\"\"Encode a string.\"\"\"\n",
    "        if \"\\n\" in v:\n",
    "            return v  # If it's a multi-line string, return it as-is\n",
    "        return super().dump_str(v)\n",
    "    \n",
    "    def dump_value(self, v):\n",
    "        \"\"\"Determine the type of a Python object and serialize it accordingly.\"\"\"\n",
    "        if isinstance(v, str) and \"\\n\" in v:\n",
    "            return '\"\"\"\\n' + v.strip() + '\\n' + '\"\"\"'\n",
    "        return super().dump_value(v)\n",
    "    \n",
    "\n",
    "class PublishedObj:\n",
    "    def __init__(self, value, metadata=None):\n",
    "        self.value = value\n",
    "        self._metadata = metadata or {}\n",
    "        \n",
    "        self._content = None\n",
    "        self._str_content = None\n",
    "\n",
    "        # Check if this is a file type and then load content if possible\n",
    "        if self._metadata.get('type') == 'file':\n",
    "            self.load_file_content()\n",
    "            \n",
    "    def __call__(self):\n",
    "        return self.value\n",
    "\n",
    "    def load_file_content(self):\n",
    "        # Check if file exists\n",
    "        if os.path.exists(self.value):\n",
    "            try:\n",
    "                with open(self.value, 'rb') as f:\n",
    "                    self._content = f.read()\n",
    "            except Exception as e:\n",
    "                printrw(f\"Error reading file: {e}\")\n",
    "\n",
    "            # If the file extension indicates it's a text type, load as string\n",
    "            if self.value.endswith(('.txt', '.csv', '.json')):  # add more text file extensions as needed\n",
    "                self._str_content = self._content.decode('utf-8', errors='replace')\n",
    "\n",
    "    @property\n",
    "    def content(self):\n",
    "        return self._content\n",
    "\n",
    "    @property\n",
    "    def str_content(self):\n",
    "        return self._str_content\n",
    "\n",
    "    @property\n",
    "    def metadata(self):\n",
    "        return self._metadata\n",
    "\n",
    "\n",
    "def check_for_embedded_objects(metadata, current_path=None, existing_results=None):\n",
    "    if existing_results is None:\n",
    "        result = {}\n",
    "    else:\n",
    "        result = existing_results.copy()\n",
    "    \n",
    "    # Initialize current_path as an empty list if it's None\n",
    "    if current_path is None:\n",
    "        current_path = []\n",
    "\n",
    "    for k, v in metadata.items():\n",
    "        new_path = current_path + [k]\n",
    "        \n",
    "        if isinstance(v, PublishedObj):\n",
    "            # Generate a key based on the current path\n",
    "            key = \".\".join(new_path)\n",
    "            \n",
    "            #printrw(f\"Found embedded object {k} at path {key}\")\n",
    "\n",
    "            # Store the metadata\n",
    "            keys = key.split('.')\n",
    "            current_result = result\n",
    "            for k in keys[:-1]:\n",
    "                if k not in current_result:\n",
    "                    current_result[k] = {}\n",
    "                current_result = current_result[k]\n",
    "            \n",
    "            current_result[keys[-1]] = v#.metadata['published_url']\n",
    "\n",
    "        elif isinstance(v, dict):\n",
    "            # check recursively\n",
    "            new_result = check_for_embedded_objects(v, current_path=new_path, existing_results=result)\n",
    "            # Merge new results into the existing result dictionary\n",
    "            result.update(new_result)\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "@requires_config\n",
    "def publish_data(content, name, metadata={}, watch=True):\n",
    "    \"\"\"\n",
    "    Save data to default pubdata.toml file and register metadata.\n",
    "    \"\"\"\n",
    "    base_config = read_base_config()\n",
    "\n",
    "    # handle any embedded objects:\n",
    "    # recurse through metadata and find any objects that are of type PublishedObj\n",
    "    # and print out their names\n",
    "    embedded_objects = check_for_embedded_objects(metadata)\n",
    "\n",
    "    if embedded_objects:\n",
    "        for k,v in embedded_objects.items():\n",
    "            if isinstance(v, PublishedObj):\n",
    "                if base_config['repro']['verbose']:\n",
    "                    printrw(f\"Found embedded object {k} at path {v}\")\n",
    "                #dynamic_data[k] = v.metadata['published_url']\n",
    "                metadata[k] = v.metadata['published_url']\n",
    "            \n",
    "            elif isinstance(v,dict):\n",
    "                for k2,v2 in v.items():\n",
    "                    if isinstance(v2, PublishedObj):\n",
    "                        if base_config['repro']['verbose']:\n",
    "                            printrw(f\"Found embedded at second level: {k}.{k2}: {v2}\")\n",
    "                        #dynamic_data[rf'\"{filename}\".{k}'] = {f'{k2}': v2.metadata['published_url']}\n",
    "                        if k in metadata:\n",
    "                            metadata[k][k2] = v2.metadata['published_url']\n",
    "                        else:\n",
    "                            metadata[k] = {k2: v2.metadata['published_url']}\n",
    "                        \n",
    "                    else:\n",
    "                        raise Exception('ISSUE HERE: Only handle one level nesting for now')\n",
    "            \n",
    "            else:\n",
    "                raise Error('Only handle one level nesting for now')\n",
    "    \n",
    "    # Capture metadata\n",
    "    \n",
    "    timestamp = datetime.datetime.now().isoformat()\n",
    "    inspect_filename = inspect.currentframe().f_back.f_code.co_filename\n",
    "    python_version = sys.version.strip().replace('\\n', ' ')\n",
    "    platform_info = platform.platform()\n",
    "\n",
    "    # generate cryptographic hash of file contents\n",
    "    content_hash = hashlib.md5(str(content).encode('utf-8')).hexdigest()\n",
    "    timed_hash = hashlib.md5((str(content) + timestamp).encode('utf-8')).hexdigest()\n",
    "         \n",
    "    # Store metadata\n",
    "    new_metadata = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"content_hash\": content_hash,\n",
    "        \"timed_hash\": timed_hash,\n",
    "        \"python_version\": python_version,\n",
    "        \"platform_info\": platform_info,\n",
    "    }\n",
    "    if VAR_REGISTRY['REPROWORK_REMOTE_URL']:\n",
    "        metadata['published_url'] = f\"{VAR_REGISTRY['REPROWORK_REMOTE_URL']}/{reproduce_dir}/pubdata.toml\"\n",
    "\n",
    "    if VAR_REGISTRY['REPROWORK_ACTIVE_NOTEBOOK']:\n",
    "        metadata['generating_script'] = VAR_REGISTRY['REPROWORK_ACTIVE_NOTEBOOK']\n",
    "    else:\n",
    "        metadata['generating_script'] = inspect_filename\n",
    "\n",
    "\n",
    "    base_config = read_base_config()\n",
    "    metadata.update(new_metadata)\n",
    "\n",
    "    if metadata.get('type', '') == 'text/latex':\n",
    "        # escape special characters\n",
    "        metadata['value'] = content.replace('\\\\', '\\\\\\\\').replace('&', '\\\\&').replace('$', '\\$')\n",
    "    elif isinstance(content, dict):\n",
    "        metadata['value'] = f'''{toml_dump(content)}'''\n",
    "    else:\n",
    "        metadata['value'] = content\n",
    "\n",
    "    if watch:\n",
    "        update_watched_files(add=[Path(reproduce_dir, 'pubdata.toml').resolve().as_posix().replace('/home/jovyan/', '')])\n",
    "\n",
    "    # check if dynamic file exists\n",
    "    if not os.path.exists(Path(base_config['repro']['files']['dynamic'])):\n",
    "        with open(Path(base_config['repro']['files']['dynamic']), 'w') as file:\n",
    "            file.write(toml.dumps({}))\n",
    "\n",
    "    with open(Path(base_config['repro']['files']['dynamic']), 'r') as file:\n",
    "        existing_dynamic_data = toml.load(file)\n",
    "        \n",
    "    dynamic_data = existing_dynamic_data.copy()\n",
    "    dynamic_data[name] = metadata\n",
    "\n",
    "    existing_vals = []\n",
    "    for k,v in existing_dynamic_data.items():\n",
    "        if isinstance(v, dict):\n",
    "            if 'value' in v:\n",
    "                existing_vals.append({k:v['value']})\n",
    "    \n",
    "    new_vals = []\n",
    "    for k,v in dynamic_data.items():\n",
    "        if isinstance(v, dict):\n",
    "            if 'value' in v:\n",
    "                new_vals.append({k:v['value']})\n",
    "\n",
    "\n",
    "    def is_equal(a, b):\n",
    "        try:\n",
    "            a = toml_dump(a)\n",
    "            b = toml_dump(b)\n",
    "            # Handle NumPy arrays\n",
    "            if isinstance(a, np.ndarray) or isinstance(b, np.ndarray):\n",
    "                return np.array_equal(a, b)\n",
    "            \n",
    "            # Handle lists and tuples\n",
    "            elif isinstance(a, (list, tuple)) and isinstance(b, (list, tuple)):\n",
    "                return len(a) == len(b) and all([is_equal(x, y) for x, y in zip(a, b)])\n",
    "            \n",
    "            # For other types, attempt a direct comparison\n",
    "            else:\n",
    "                return a == b\n",
    "            \n",
    "        except ValueError:\n",
    "            # If direct comparison raises a ValueError, assume they're not equal\n",
    "            printrw('ValueError: could not compare values: {} and {}'.format(a, b))\n",
    "            return False\n",
    "\n",
    "    #value_changed = any([not is_equal(v, new_vals[i]) for i, v in enumerate(existing_vals)])\n",
    "    #iterate through existing_vals and new_vals and check if any have changed\n",
    "    changed_vals = []\n",
    "    for i, v in enumerate(existing_vals):\n",
    "        if i<len(new_vals):\n",
    "            #if objects are identical, return False; otherwise return name(s) of variable(s) that changed\n",
    "            if not is_equal(v, new_vals[i]):\n",
    "                changed_vals.append(name)\n",
    "\n",
    "    # do same for reverse\n",
    "    for i, v in enumerate(new_vals):\n",
    "        if i<len(existing_vals):\n",
    "            if not is_equal(v, existing_vals[i]):\n",
    "                changed_vals.append(name)\n",
    "    \n",
    "    changed_vals = list(set(changed_vals))\n",
    "\n",
    "    if changed_vals:\n",
    "        value_changed = changed_vals\n",
    "    else:\n",
    "        value_changed = False\n",
    "    \n",
    "\n",
    "\n",
    "    existing_nontimefields = {name: {k:v for k,v in var.items() if (isinstance(k, str) and k[:4] not in ['time','valu'])} if isinstance(var, dict) else var for name,var in existing_dynamic_data.items()}\n",
    "    new_nontimefields = {name: {k:v for k,v in var.items() if (isinstance(k, str) and k[:4] not in ['time','valu'])} if isinstance(var, dict) else var for name,var in dynamic_data.items()}\n",
    "    which_changed = []\n",
    "    \n",
    "    if existing_nontimefields == new_nontimefields:\n",
    "        non_timefield_changed = False\n",
    "    else:\n",
    "        for name in existing_nontimefields:\n",
    "            if name in new_nontimefields:\n",
    "                if existing_nontimefields[name] != new_nontimefields[name]:\n",
    "                    which_changed.append(name)\n",
    "            else:\n",
    "                which_changed.append(name)\n",
    "        \n",
    "        for name in new_nontimefields:\n",
    "            if name not in existing_nontimefields:\n",
    "                which_changed.append(name)\n",
    "        \n",
    "        non_timefield_changed = which_changed\n",
    "\n",
    "    if value_changed:\n",
    "        which_changed = value_changed + which_changed\n",
    "        \n",
    "    if value_changed or non_timefield_changed:\n",
    "        with open(Path(base_config['repro']['files']['dynamic']), 'w') as file:\n",
    "            toml.dump(dynamic_data, file, encoder=ReproduceWorkEncoder())\n",
    "        \n",
    "        if base_config['repro']['verbose']:\n",
    "            if len(which_changed)>1:\n",
    "                printrw(f\"Updated {which_changed} in {base_config['repro']['files']['dynamic']}\")\n",
    "            else:\n",
    "                printrw(f\"Updated {which_changed[0]} in {base_config['repro']['files']['dynamic']}\")\n",
    "\n",
    "    data_obj = PublishedObj(metadata['value'], metadata)\n",
    "    return data_obj\n",
    "                 \n",
    "@requires_config\n",
    "def publish_file(filename, metadata={}, watch=True):\n",
    "    \"\"\"\n",
    "    Save content to a file and register metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_config = read_base_config()\n",
    "\n",
    "    # handle any embedded objects:\n",
    "    # recurse through metadata and find any objects that are of type PublishedObj\n",
    "    # and print out their names\n",
    "    embedded_objects = check_for_embedded_objects(metadata)\n",
    "\n",
    "    if embedded_objects:\n",
    "        for k,v in embedded_objects.items():\n",
    "            if isinstance(v, PublishedObj):\n",
    "                if base_config['repro']['verbose']:\n",
    "                    printrw(f\"Found embedded object {k} at path {v}\")\n",
    "                #dynamic_data[k] = v.metadata['published_url']\n",
    "                metadata[k] = v.metadata['published_url']\n",
    "            \n",
    "            elif isinstance(v,dict):\n",
    "                for k2,v2 in v.items():\n",
    "                    if isinstance(v2, PublishedObj):\n",
    "                        if base_config['repro']['verbose']:\n",
    "                            printrw(f\"Found embedded at second level: {k}.{k2}: {v2}\")\n",
    "                        #dynamic_data[rf'\"{filename}\".{k}'] = {f'{k2}': v2.metadata['published_url']}\n",
    "                        if k in metadata:\n",
    "                            metadata[k][k2] = v2.metadata['published_url']\n",
    "                        else:\n",
    "                            metadata[k] = {k2: v2.metadata['published_url']}\n",
    "                        \n",
    "                    else:\n",
    "                        raise Exception('ISSUE HERE: Only handle one level nesting for now')\n",
    "            \n",
    "            else:\n",
    "                raise Error('Only handle one level nesting for now')\n",
    "\n",
    "\n",
    "    for k,v in metadata.items():\n",
    "        if isinstance(v, dict):\n",
    "            #printrw('Dumping dict w/ toml')\n",
    "            metadata[k] = rf'{toml_dump(v)}'\n",
    "\n",
    "    # Capture metadata\n",
    "    timestamp = datetime.datetime.now().isoformat()\n",
    "    inspect_filename = inspect.currentframe().f_back.f_code.co_filename\n",
    "    #python_version = sys.version.strip().replace('\\n', ' ')\n",
    "    #platform_info = platform.platform()\n",
    "\n",
    "    # generate cryptographic hash of file contents\n",
    "\n",
    "    with open(filename, 'rb') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    content_hash = hashlib.md5(content).hexdigest()\n",
    "    timed_hash = hashlib.md5((content_hash + timestamp).encode('utf-8')).hexdigest()\n",
    "         \n",
    "    #save_context, definition_context = check_for_defintion_in_context(function_name='save')\n",
    "\n",
    "    # Store metadata\n",
    "    new_metadata = {\n",
    "        \"type\": \"file\",\n",
    "        \"timestamp\": timestamp,\n",
    "        #\"python_version\": python_version,\n",
    "        #\"platform_info\": platform_info,\n",
    "        \"content_hash\": content_hash,\n",
    "        \"timed_hash\": timed_hash,\n",
    "        #\"save_context\": save_context,\n",
    "        #\"definition_context\": definition_context\n",
    "    }\n",
    "    cell_index = get_cell_index()\n",
    "    if cell_index:\n",
    "        new_metadata[\"cell_index\"] = cell_index\n",
    "\n",
    "    if VAR_REGISTRY['REPROWORK_REMOTE_URL']:\n",
    "        new_metadata['published_url'] = f\"{VAR_REGISTRY['REPROWORK_REMOTE_URL']}/{filename}\"\n",
    "\n",
    "    if VAR_REGISTRY['REPROWORK_ACTIVE_NOTEBOOK']:\n",
    "        new_metadata['generating_script'] = VAR_REGISTRY['REPROWORK_ACTIVE_NOTEBOOK']\n",
    "    else:\n",
    "        new_metadata['generating_script'] = inspect_filename\n",
    "\n",
    "    \n",
    "    metadata.update(new_metadata)\n",
    "\n",
    "    if watch:\n",
    "        update_watched_files(add=[filename])\n",
    "\n",
    "    # check if dynamic file exists\n",
    "    if not os.path.exists(Path(base_config['repro']['files']['dynamic'])):\n",
    "        with open(Path(base_config['repro']['files']['dynamic']), 'w') as file:\n",
    "            file.write(toml.dumps({}))\n",
    "\n",
    "    with open(Path(base_config['repro']['files']['dynamic']), 'r') as file:\n",
    "        dynamic_data = toml.load(file)\n",
    "\n",
    "    dub_quot_,sing_quot_ =r'\"{filename}\"' in dynamic_data, rf\"'{filename}'\" in dynamic_data\n",
    "    if dub_quot_ or sing_quot_:\n",
    "        printrw(f\"Overwriting existing metadata for file {filename}\")\n",
    "        existing_dyndata = dynamic_data.pop(dub_quot_ if dub_quot_ else sing_quot_)\n",
    "        existing_dyndata.update(metadata)\n",
    "        metadata = existing_dyndata\n",
    "\n",
    "    dynamic_data[filename] = metadata\n",
    "\n",
    "    printrw(dynamic_data)\n",
    "\n",
    "    with open(Path(base_config['repro']['files']['dynamic']), 'w') as file:\n",
    "        toml.dump(dynamic_data, file, encoder=ReproduceWorkEncoder())\n",
    "\n",
    "    if 'verbosity' in base_config['repro'] and base_config['repro']['verbose']:\n",
    "        printrw(f\"Added metadata for file {filename} to dynamic file {base_config['repro']['files']['dynamic']}\")\n",
    "\n",
    "    data_obj = PublishedObj(filename, metadata)\n",
    "    return data_obj\n",
    "\n",
    "\n",
    "\n",
    "def reproducible(var_assignment_func):\n",
    "    \"\"\"\n",
    "    A decorator to register the line number and timestamp when a variable is assigned.\n",
    "    \"\"\"\n",
    "    @functools.wraps(var_assignment_func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Extract value and var_name from args\n",
    "        # Assumes the decorated function always takes at least two arguments: value and var_name\n",
    "        value, var_name = args[0], args[1]\n",
    "\n",
    "        # Extract metadata from kwargs or default to an empty dictionary\n",
    "        metadata = kwargs.get('metadata', {})\n",
    "\n",
    "        # Get the current frame and line number\n",
    "        frame = inspect.currentframe()\n",
    "        line_number = frame.f_back.f_lineno\n",
    "\n",
    "        # Get the current timestamp\n",
    "        timestamp = datetime.datetime.now().isoformat()\n",
    "\n",
    "        # Get the filename of the caller\n",
    "        filename = frame.f_back.f_code.co_filename\n",
    "\n",
    "        # Execute the variable assignment function\n",
    "        result = var_assignment_func(*args, **kwargs)\n",
    "\n",
    "        # Register the variable name, line number, timestamp, and filename\n",
    "        VAR_REGISTRY[var_name] = {\n",
    "            \"type\": \"string\",\n",
    "            \"timestamp\": timestamp,\n",
    "        }\n",
    "\n",
    "        if type(value) is not str:\n",
    "            value = str(value)\n",
    "            printrw(f\"WARNING: value of {var_name} was not a string. Converted to string: {value}.\")\n",
    "\n",
    "        VAR_REGISTRY[var_name]['value'] = value\n",
    "\n",
    "        metadata.update(VAR_REGISTRY[var_name])\n",
    "        \n",
    "        if VAR_REGISTRY['REPROWORK_REMOTE_URL']:\n",
    "            metadata['published_url'] = f\"{VAR_REGISTRY['REPROWORK_REMOTE_URL']}/{reproduce_dir}/pubdata.toml\"\n",
    "\n",
    "        if VAR_REGISTRY['REPROWORK_ACTIVE_NOTEBOOK']:\n",
    "            metadata['generating_script'] = VAR_REGISTRY['REPROWORK_ACTIVE_NOTEBOOK']\n",
    "\n",
    "        config = read_base_config()\n",
    "\n",
    "        # check if dynamic file exists\n",
    "        if not os.path.exists(Path(config['repro']['files']['dynamic'])):\n",
    "            with open(Path(config['repro']['files']['dynamic']), 'w') as file:\n",
    "                file.write(toml.dumps({}))\n",
    "        with open(Path(config['repro']['files']['dynamic']), 'r') as file:\n",
    "            dynamic_data = toml.load(file)\n",
    "\n",
    "        dynamic_data[var_name] = metadata\n",
    "\n",
    "        with open(Path(config['repro']['files']['dynamic']), 'w') as file:\n",
    "            toml.dump(dynamic_data, file, encoder=ReproduceWorkEncoder())\n",
    "\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@reproducible\n",
    "def publish_variable(value, var_name, metadata={}):\n",
    "    globals()[var_name] = value\n",
    "\n",
    "\n",
    "@requires_config\n",
    "def register_notebook(notebook_name, notebook_dir='nbs', quiet=False):\n",
    "    \"\"\"\n",
    "    Register a notebook to the config.toml file.\n",
    "    \"\"\"\n",
    "    notebook_path = notebook_dir + '/' + notebook_name\n",
    "    base_config = read_base_config()\n",
    "    \n",
    "    # ensure notebook key exists\n",
    "    if 'notebooks' not in base_config['repro']:\n",
    "        base_config['repro']['notebooks'] = []\n",
    "\n",
    "    if notebook_path not in base_config['repro']['notebooks']:\n",
    "        base_config['repro']['notebooks'].append(notebook_path)\n",
    "        with open(Path(reproduce_dir, 'config.toml'), 'w') as f:\n",
    "            toml.dump(base_config, f, encoder=ReproduceWorkEncoder())\n",
    "            \n",
    "    else:\n",
    "        if base_config['repro']['verbose']:\n",
    "            printrw(f\"Notebook {notebook_path} already registered in {reproduce_dir}/config.toml\")\n",
    "\n",
    "    update_watched_files(add=[notebook_path], quiet=True)\n",
    "\n",
    "    if 'github_repo' in base_config['project']:\n",
    "        remote_url_val = f\"https://github.com/{base_config['project']['github_repo']}/blob/main\"\n",
    "        notebook_new_val = f\"{remote_url_val}/{notebook_path}\"\n",
    "    else:\n",
    "        notebook_new_val = Path(notebook_path).resolve().as_posix()\n",
    "    \n",
    "\n",
    "    if VAR_REGISTRY['REPROWORK_REMOTE_URL']:\n",
    "        printrw(f\"Warning: {VAR_REGISTRY['REPROWORK_REMOTE_URL']} is already registered. Overwriting with {remote_url_val}\")\n",
    "    VAR_REGISTRY['REPROWORK_REMOTE_URL'] = remote_url_val\n",
    "\n",
    "    if VAR_REGISTRY['REPROWORK_ACTIVE_NOTEBOOK']:\n",
    "        printrw(f\"Warning: Notebook {VAR_REGISTRY['REPROWORK_ACTIVE_NOTEBOOK']} is already registered. Overwriting with {notebook_new_val}\")\n",
    "    VAR_REGISTRY['REPROWORK_ACTIVE_NOTEBOOK'] = notebook_new_val\n",
    "\n",
    "    if base_config['repro']['verbose']:\n",
    "        printrw(f\"Registered notebook {notebook_new_val} in {reproduce_dir}/config.toml\")\n",
    "    #return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'REPROWORK_REMOTE_URL': None, 'REPROWORK_ACTIVE_NOTEBOOK': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAR_REGISTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔ω: Registered notebook https://github.com/reproduce-work/reproduce-work/blob/main/nbs/01_core.ipynb in reproduce/config.toml\n"
     ]
    }
   ],
   "source": [
    "register_notebook('01_core.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔ω: Updated simple_string in reproduce/pubdata.toml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.PublishedObj at 0xffff6670abb0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publish_data(\"Hello world!\", \"simple_string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the serialize_to_toml function\n",
    "data_sample = {\n",
    "    'name': 'John',\n",
    "    'age': 28,\n",
    "    'is_student': False,\n",
    "    'scores': [85, 90, 78, 92],\n",
    "    'birthday': pd.Timestamp('2000-01-01'),\n",
    "    'matrix': np.array([[1, 2], [3, 4]]),\n",
    "    'df': pd.DataFrame({\n",
    "        'A': [1, 2, 3],\n",
    "        'B': ['a', 'b', 'c'],\n",
    "        'date': [pd.Timestamp('2022-01-01'), pd.Timestamp('2022-01-02'), pd.Timestamp('2022-01-03')]\n",
    "    }),\n",
    "    'nested_dict': {\n",
    "        'key1': 'value1',\n",
    "        'sub_dict': {\n",
    "            'sub_key': 'sub_value'\n",
    "        }\n",
    "    },\n",
    "    'none_value': None\n",
    "}\n",
    "\n",
    "#publish_data(data_sample, \"data_sample\")  # This should capture this line number and timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔ω: Updated data_sample in reproduce/pubdata.toml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.PublishedObj at 0xffff66759b20>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# publishing the exact same data twice will NOT update the pubdata.toml file\n",
    "rw_data_sample = publish_data(data_sample, \"data_sample\")\n",
    "rw_data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
