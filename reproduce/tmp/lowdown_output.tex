
\hypertarget{introduction.}{%
\section{Introduction.}\label{introduction.}}

In metascience, computational reproduction is the process of reproducing the results of a scientific paper using the data and code provided by the authors of the paper. This subject sits within the broader context of ``reproducibility'' in scientific  research, which is the idea that scientific results should be reproducible by other scientists (or anyone interested, for that matter). Concepts around reproducibility have been core to the philosophy of science for decades, but several aspects of the scientific method have been challenged by recent developments. 
First, recent conversations around \$p\$-hacking and fraud in scientific publishing have led to a ``crisis'' at the core of of social science centered around the role of human error, incentives, and guile    . 
Additionally , the increasing computational complexity of scientific research has led to a new set of challenges. Chief among these is the fact that the results of many scientific papers are not reproducible to even the lowest degree. This is because the data and code used to produce the results are not made available to the public.


\textbackslash{}begin\{tikzpicture\}[node distance=1.5cm, auto]

\% Document Nodes (Rectangles)
\textbackslash{}node[draw, rectangle, minimum width=2.5cm, minimum height=1.5cm] (reportTentative) \{report.tentative\};
\textbackslash{}node[draw, rectangle, minimum width=2.5cm, minimum height=1.5cm, right=of reportTentative] (reportReproduce) \{report.reproduce\};
\textbackslash{}node[draw, rectangle, minimum width=2.5cm, minimum height=1.5cm, right=of reportReproduce] (pubdata) \{pubdata.toml\};
\textbackslash{}node[draw, rectangle, minimum width=2.5cm, minimum height=1.5cm, right=of pubdata] (config) \{config.toml\};

\% Processor Nodes (Circle)
\textbackslash{}node[draw, circle, below=1cm of pubdata] (engine) \{
\textbackslash{}shortstack\{
\textbackslash{}texttt\{reproduce.work\}\textbackslash{}\textbackslash{}
\{engine\}
\}
\};

\% Scientific Report Node (Rectangle)
\textbackslash{}node[draw, rectangle, minimum width=2.5cm, minimum height=1.5cm, below=of engine, xshift=-1.25cm] (outputFile) \{\textbackslash{}shortstack\{\texttt{output\textbackslash{}\_file:}\textbackslash{}\textbackslash{} \{\texttt{report.pdf}\}\}\};
\textbackslash{}node[draw, rectangle, minimum width=2.5cm, minimum height=1.5cm, right=of outputFile] (report) \{verification standards\};

\% Arrows
\textbackslash{}draw[->] (reportReproduce) -- (engine);
\textbackslash{}draw[->] (pubdata) -- (engine);
\textbackslash{}draw[->] (config) -- (engine);
\textbackslash{}draw[->] (engine) -- (report);

\textbackslash{}end\{tikzpicture\}

\hypertarget{barriers-to-computational-reproduction}{%
\subsection{Barriers to computational reproduction}\label{barriers-to-computational-reproduction}}

\textbf{Lack of documentation}. Very rarely is it ever written down explicitly which code was run to generate which results in a scientific report. Even if the code is open and published by the authors, it may be complex and difficult to parse. Scientific code is often written with the intention of generating specific set of results, rather than with the intention of being read and reproduced independently. As such, the code is often not documented in a way that is easy to understand.

\textbf{Platform dependence.} The reality is that is not enough to merely point to the code that generated a given piece of data. For even if a human were to track down the relevant commands, setting up the requisite computing environment to reproduce the results is often a non-trivial task. This is because the code that produced the results may depend on a specific version of a programming language, a specific version of a software package, or even a specific operating system.

\textbf{Code complexity}. Even if the computing environment is set up correctly, the code may be too complex to understand. This is especially true for code that is written by a researcher who is not a professional software developer. One script may ``generate'' a resulting variable for publication, but this script alone may depend on several other scripts and files in the process. Very rarely are complex scientific analyses possible to reproduce by running a simple function or script with a single command.

\hypertarget{background}{%
\section{Background}\label{background}}

advent of computational science has brought new challenges to the field.

At the heart of the concern with research by ousted Stanford president Mark is the fact that it is impossible to verify whether the images produced in the paper are fabricated or actually generated by the scientific equipment they claimed to use in the published manuscript.

The problem of reproducibility is not limited to the field of psychology. 

Reproducibility across the disciplines:
In 2012, the pharmaceutical company Amgen attempted to reproduce the results of 53 landmark cancer studies. They were only able to reproduce the results of 6 of the 53 studies.

In 2015, the journal Science published a study that attempted to reproduce the results of 100 psychology studies. They were only able to reproduce the results of 36 of the 100 studies.

In 2016, the journal Nature published a study that attempted to reproduce the results of 5 landmark cancer studies. They were only able to reproduce the results of 2 of the 5 studies.

In 2018, the journal Science published a study that attempted to reproduce the results of 21 landmark economics studies. They were only able to reproduce the results of 1 of the 21 studies.

this is known as the \href{https://en.wikipedia.org/wiki/Replication\_crisis}{replication crisis}.

for real?

\hypertarget{related-projects}{%
\subsection{Related projects}\label{related-projects}}

There have been a number of projects with similar aims to this one.

Some work has been done to develop reproducible workflows within specific computing environments. See \href{https://mine-cetinkaya-rundel.github.io/improve-repro-workflow-reproducibilitea-2020/}{example 1} in R.

While some initiatives have emerged to facilitate computational reproduction, there is no widely accepted standard for computational reproduction. 

There do exist efforts around the ideas of open science, which are closely related but distinct from reproducible science. See \href{https://osf.io/tvyxz/}{OSF}, for example.

\hypertarget{an-ontology-of-computation-and-composition}{%
\subsection{An ontology of computation and composition}\label{an-ontology-of-computation-and-composition}}

\textbackslash{}begin\{figure\}[h]
\textbackslash{}centering
\textbackslash{}caption\{Different models of ontology of computation and composition.\}
\textbackslash{}label\{fig:comp\}
\textbackslash{}includegraphics[width=\textbackslash{}textwidth]\{../../nbs/img/comp.pdf\}

\textbackslash{}vspace\{2mm\}
\textbackslash{}begin\{minipage\}\{0.95\textbackslash{}textwidth\}
\%\textbackslash{}centering
\textbackslash{}setstretch\{1\}
\textbackslash{}footnotesize\{
\textbackslash{}emph\{Note:\} The \textbf{interweave} model is one in which computation and composition happen linearly within the flow of a single document. Of course, many paradigms including RMarkdown and MyST are quite flexible and allow for a variety of workflows; however, we believe most existing software follows this model. This leads to our proposal of the \textbf{tracked model with structured interfacing}. In this model, the data and code are tracked in a way that allows for easy verification and traceability of reported scientific results with structured metadata.
\}
\textbackslash{}end\{minipage\}
\textbackslash{}end\{figure\}

Lay readers may be surprised to learn that the results of many scientific papers are not reproducible to even the lowest degree. (We put forward the following standard as the first of computational reproducibility: ``More than one person verified that the results in the published paper match the results of the code and data provided by the authors.'') 

This is because the data and code used to produce the results are not made available to the public. 

such as the \href{https://rescience.github.io/}{ReScience journal} and \href{https://jupyterbook.org/en/stable/content/myst.html}{MyST Markdown}

While many advocate for the value of peer review in scientific publishing, as of 2023, there no scientific standard exists for true computational reproducibility. 

This is a problem because it means that the quality of peer review varies from journal to journal, and even from paper to paper. 

Do i include \textbackslash{}hyperlink\{reproduce.work\}\{0.068\}?

the most promising avenue might be emphasizing the importance of collaboration and transparency. By fostering a culture where researchers work together, openly share data and findings, and commit to thorough documentation, the reliability and replicability of work can be increased.

In the coming era, science must not be buoyed by computational power alone, but also by a culture of rigor, collaboration, and transparency.
, the challenge lies not just in advancing science but in ensuring its very foundation remains robust, trustworthy, and verifiable. 

\hypertarget{software}{%
\section{Software}\label{software}}

\hypertarget{seemless-reproducible-scientific-computing-via-containerization}{%
\subsection{Seemless reproducible scientific computing via containerization}\label{seemless-reproducible-scientific-computing-via-containerization}}

Scientific computing is very much like other computing, so it should come as no surprise that tools that have been developed to faciltate the reproduction of arbitrary computing tasks can be adapted to the ends of scientists. Containerization is a technology that allows for the creation of self-contained computing ``containers'' that can be run on any computer with the containerization software installed. While this does necessitate the installation of the containerization software, this is a one-time task that can be done by anyone with a computer. Once the containerization software is installed, the promise of containerization is that a container can be run on any computer, regardless of the operating system or other software installed on the computer. Many containerization standards exist; for present purposes this project uses the \href{https://www.docker.com/}{Docker} containerization standard.

\hypertarget{init}{%
\subsubsection{init}\label{init}}

\hypertarget{develop}{%
\subsubsection{develop}\label{develop}}

\hypertarget{build}{%
\subsubsection{build}\label{build}}

\hypertarget{verify}{%
\subsubsection{verify}\label{verify}}

\hypertarget{the-reproduce.work-workflow}{%
\subsection{The reproduce.work workflow}\label{the-reproduce.work-workflow}}

The workflow put forward in this project is designed to be as simple as possible while accommodating a wide variety of scientific output. 

\begin{enumerate}
\itemsep -0.2em
\item Clone reproduce-work/template repository
\item Build and run the containerized interface; within the container:

\begin{itemize}
\itemsep -0.2em
\item Use specialized software, add data, and conduct analysis
\item reproduce.work software facilitates publishing of standardized computational report data with structured metadata
\end{itemize}
\item Verify compliance to open and reproducibility standards and publish project to the web
\end{enumerate}

In the context of this version, ``verify'' will mean only to verify the computation reproduction of a given document. In future versions, we hope to expand this scope to include verification of the data collection and other analysis steps. On a more practical level, what this means is that for each piece of data or statistical output reported in a published document, we will verify two things:

\begin{enumerate}
\itemsep -0.2em
\item That the data and code provided by the authors of the document produce the same output as the published document.
\item That each piece of data and statistical output reported in the published document is documented with structured metadata to trace the origin of the result and provide a link to the code that produced the result.
\end{enumerate}

\hypertarget{future-directions}{%
\section{Future directions}\label{future-directions}}

In this section we mention some of the ambitious and yet under-developed ideas for future directions for this project.

\hypertarget{computational-reproducibility-with-pre-registration}{%
\subsection{Computational reproducibility with pre-registration}\label{computational-reproducibility-with-pre-registration}}

\hypertarget{pre-registered-code-with-verifiable-order-of-computation}{%
\subsubsection{Pre-registered code with verifiable order-of-computation}\label{pre-registered-code-with-verifiable-order-of-computation}}

\$\$ \textbackslash{}text\{pre-registered code\} \textbackslash{}rightarrow \textbackslash{}text\{timestamped data collection\} \textbackslash{}rightarrow \textbackslash{}text\{code execution\} \textbackslash{}rightarrow \textbackslash{}text\{verified results\} \$\$

\hypertarget{adversarial-collaborations.}{%
\subsubsection{Adversarial collaborations.}\label{adversarial-collaborations.}}

A key requisite for successful adversarial collaborations is the ability to agree on code and data structure and standards. Being able to easily verify that results correspond to code that was written before but executed after the collection of data.

\hypertarget{computational-reproducibility-with-specification-curves-or-ml-driven-analysis}{%
\subsection{Computational reproducibility with specification curves or ML-driven analysis}\label{computational-reproducibility-with-specification-curves-or-ml-driven-analysis}}

One may ask whether the framework here prevents questionable research practices including undesirable behavior like \$p\$-hacking or seed hacking. Unfortunately, the framework put forward here is not a panacea. The bar we set out to clear in this version of the project was to take steps toward a world where computational reproduction is possible. Developing software specifically designed to discourage bad or encourage good aspects of research behavior is a task for future work. Nonetheless, it is valuable to discuss some ideas in which we may use our computing paradigm to encourage certain behaviors and practices of quantitative research.

One such method for exploration in future versions of this work is the potential for pre-registering exactly the model specification with which one wishes to estimate. If we were forced to pre-register our exact model specifications, we would be discouraged from pursuing models that depend on researcher degrees of freedom that can be exploited between the collection of data and reporting of results. As such, this may encourage scientists to explore the use of machine learning for analysis of heterogeneous treatment effects or specification curve analyses that are designed to be robust to specification searching. 

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

The project put forward in the body of this project is a mere first step in the direction of computational reproduction. The software lacks support for many important features and the concept is heretofore untested. While there are many aspects to reproducibility in the epistemology of science, we believe that computational reproducibility is a necessary first step toward a world where scientific results are verifiable and trustworthy. However, given the low bars required to improve the existing practices around computational reproduction, we beleive this project and the ideas put forward here have potential as the seed of a growing culture of scientific rigor and transparency. 
